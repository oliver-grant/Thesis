\documentclass[]{beamer}

\usepackage{beamerthemesplit} 
\usepackage{tikz}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{pifont}
\usepackage{amsmath}
\usepackage{amssymb}

\usepackage{enumerate}
\usepackage{amsfonts}
\usepackage{url}
\usepackage{amsmath,amsfonts,amssymb,amsthm,epsfig,epstopdf,url,array}
\usepackage[]{units}
\usepackage{xcolor}
\usepackage{fancybox}
\usepackage{tikz}
\usepackage{stmaryrd}
\usepackage{caption}
\usepackage{subcaption}
\theoremstyle{plain}
\newtheorem{thm}{Theorem}[section]
\newtheorem{lem}[thm]{Lemma}
\newtheorem{prop}[thm]{Proposition}
\newtheorem*{cor}{Corollary}

\newcommand*\rfrac[2]{{#1}/{#2}}
\makeatletter
\newcommand{\rom}[1]{\romannumeral #1}

\definecolor{RazerGreen}{RGB}{71,225,12}
\definecolor{RazerGreen}{RGB}{71,225,12}
\setbeamercolor{title}{fg=black}
\setbeamercolor{frametitle}{fg=black}
\setbeamercolor{structure}{fg=RazerGreen}

\tikzset{
    current/.style={
		scale=0.8,
		fill=blue,
		minimum size=0.5pt,
        draw,
        circle,
    }
}
\tikzset{
    select/.style={
		scale=0.8,
		fill=red,
		minimum size=0.5pt,
        draw,
        circle,
    }
}
\tikzset{
    max/.style={
		scale=0.8,
		minimum size=0.5pt,
        draw,
        circle,
    }
}

\tikzset{
  treenode/.style = {align=center, inner sep=0pt, text centered,
    font=\sffamily\small},
  arn_n/.style = {treenode, circle, draw=black,
    fill=white, text width=2em},% arbre rouge noir, noeud noir
  arn_small/.style = {treenode, circle, draw=black,
    fill=white, text width=2em, font=\tiny, align=center},
  arn_small_r/.style = {treenode, rectangle, draw,
    fill=white, font=\tiny, align=center},
  arn_r/.style = {treenode, circle, red, draw=red, 
    text width=2em, very thick},% arbre rouge noir, noeud rouge
  arn_x/.style = {treenode, rectangle, draw=black,
    minimum width=2.2em, minimum height=2.5em}% arbre rouge noir, nil
}

\title{Approximately Optimum Search
Trees in External Memory Models}
\subtitle{Master of Mathematics Presentation}
\author{Oliver Grant \\
Supervisor: Ian Munro}



\begin{document} \date{}
\begin{frame}
  \titlepage
\end{frame}

%\tableofcontents

\section{Introduction}

\begin{frame} \frametitle{Binary Search Trees}

\begin{itemize}

\item \textbf{BST} - Simple structure used to store key-value pairs

\item First appeared in the late 1950s/early 1960s

\item Attributed to Windley, Booth, Colin and Hibbard \cite{windley1960trees, booth1960efficiency, hibbard1962some}

\item Ordering property over keys allows for quick searches

\end{itemize}
\end{frame}

\begin{frame} \frametitle{The Optimum Binary Search Tree Problem}

\begin{itemize}

\item Knuth proposed optimum BST problem in 1971 \cite{knuth1971optimum}

\item $n$ keys, $B_1, B_2, ..., B_n$

\item $2n+1$ frequencies, ${p_1, p_2, ..., p_n}$, ${q_0, q_1, ..., q_n}$

\item $p_i$, probability of searching for $B_1$

\item $q_i$, probability of searching for key in gap $(B_i, B_{i+1})$

\item We assume $\sum\limits_{i=1}^n p_i + \sum\limits_{i=0}^n q_i = 1$

\item Also assume w.l.o.g. $q_{i-1}+p_i+q_i \neq 0$

\item Keys must be internal nodes, gaps leaves

\item $P = \sum_{i=1}^{n} p_i \cdot (d_T(B_i)+1) + \sum_{i=0}^{n} q_i \cdot(d_T((B_i, B_{i+1})))$


\end{itemize}
\end{frame}


\begin{frame} \frametitle{Optimum BST Example}
\begin{center}
\begin{tikzpicture}[level/.style={sibling distance = 6cm/####1,
  level distance = 1.5cm}] 
\node [arn_n] {0.15 THE}
    child{ node [arn_n] {0.10 BE} 
            child{ node [arn_x] {0.10 \\ $x < BE$}} 
            	child{ node [arn_x] {0.40 \\  $BE < x < THE$}}
            }
    child{ node [arn_n] {0.10 TO}
            child{ node [arn_x] {0.01 \\ $THE < x < TO$}}
            child{ node [arn_x] {0.14 \\ $x > TO$}}
		}
		
; 
\end{tikzpicture}



{\scriptsize
\begin{align*}
P = &\sum_{i=1}^{n} p_i \cdot (d_T(B_i)+1) + \sum_{i=0}^{n} q_i \cdot(d_T((B_i, B_{i+1}))) \\
= &0.15 \cdot 1 + 0.10 \cdot 2 + 0.10 \cdot 2 + 0.10 \cdot 2 + 0.10 \cdot 2 + 0.40 \cdot 2 + 0.01 \cdot 2 + 0.14 \cdot 2 \\
= &1.85
\end{align*}
}%

\end{center}
\end{frame}

\begin{frame} \frametitle{Three-Way Branching}

\begin{center}
\begin{tikzpicture}
\node[arn_n] {0.4 a}
  child {node [arn_n] {0.3 b}    
  }  
  child {node [arn_n] {0.3 c}
  };
\end{tikzpicture}
\end{center}

\begin{itemize}



\item \definecolor{lightgrey}{rgb}{0.95,0.92,0.92} % Defines the color used for content box headers
\colorbox{lightgrey}{ \fontfamily{cmtt}\selectfont \uppercase{IF (expr) label1, label2, label3} } 

\item Control transferred to one of $3$ locations with one statement

\item Always single comparison under three-way branching

\item Minimum $1.6$ comparisons expected using $<, >, =$
\end{itemize}
\end{frame}

\iffalse

\begin{frame} \frametitle{Why Study Binary Search Trees}


\begin{itemize}
\item AVL trees self-balancing BST in 1963, maintains $O(\lg(n))$ height \cite{adelsonvelskii1963algorithm}

\item B-trees/Red-black trees by R. Bayer in 1972 (with later extensions by others) \cite{bayer1972symmetric}

\item  Allen and Munro examined a MTR heuristic \cite{allen1978self}

\item Splay trees of Sleator and Tarjan in 1985, dynamic optimality conjecture \cite{sleator1985self}

\item Tango trees in 2007 by Demaine et al., $O(\lg \lg n)$-competitive \cite{demaine2007dynamic}

\end{itemize}

\end{frame}
\fi

\begin{frame} \frametitle{Why Study Binary Search Trees}

\begin{itemize}

\item Binary space partitions in 3D graphics \cite{schumacker1969study, paterson1992optimal}

\item Binary tries in routers and IP lookup structures \cite{song2010building}

\item Programming languages like C++ \texttt{std::map} \cite{CppMap} 

\item Syntax trees during compilation \cite{louden1997compiler}

\item Many more examples

\item Theoretically interesting

\end{itemize}

\end{frame}


\begin{frame} \frametitle{Overview}

\begin{itemize}

\item Review work on BSTs, multiway trees, and alphabetic trees

\item We have three separate results:
\end{itemize}

\begin{enumerate}
\item In the standard problem, we improve bound on MME Heuristic of G{\"u}ttler, Mehlhorn and Schneider \cite{guttler1980binary}

\item Under Hierarchical Memory Model \cite{aggarwal1987model}, two $O(n)$ time approximation algorithms

\item Given unordered probabilities show $O(n)$ algorithm within $\frac{n+1}{2n}$ of optimal
\end{enumerate}




\end{frame}

\section{Background and Related Work} \label{Background and Related Work}

\begin{frame} \frametitle{Optimum Binary Search Trees}

\begin{itemize}
\item In 1971 C. Gotlieb and Walker approximate solution \cite{walker1971top}

\item Knuth $O(n^2)$ time and space solution \cite{knuth1971optimum}

\item Approximate solutions followed

\item Interestingly, it is hard to bound the cost of approximation algorithms to the optimal

\item Discuss in terms of \textit{entropy}:
\end{itemize}

\begin{align*}
H = \sum_{i=1}^{n} p_i\cdot\lg(\frac{1}{p_i}) + \sum_{j=0}^{n} q_i\cdot\lg(\frac{1}{q_j})
\end{align*}

\end{frame}

\begin{frame} \frametitle{Approximately Optimum Binary Search Trees}


\begin{itemize}
\item In 1975, P. Bayer showed \cite{bayer1975improved}:
\begin{align*}
H-\lg H-(\lg e-1) \leq C_{Opt} \leq C_{WB}, C_{MM} \leq H + 2
\end{align*}

\item $C_{Opt}$ optimal solution

\item $C_{WB}, C_{MM}$ both top-down greedy approaches running in $O(n)$

\item $P_L(B_i)$,  $P_R(B_i)$ probabilities of searching for key before or after $B_i$ respectively

\item $C_{WB}$ weight-balanced heuristic: \\
Select root $B_i$ as root with minimum $|P_L(B_i)-P_R(B_i)|$

\item $C_{MM}$ min-max heuristic:  \\
Select $B_i$ as root with minimum $\max(P_L(B_i), P_R(B_i))$


\end{itemize}

\end{frame}

\begin{frame} \frametitle{Approximately Optimum Binary Search Trees}

\begin{itemize}

\item In addition to using $H$ to represent the entropy of our whole distribution:
\begin{center}
$H(x_1,x_2,...,x_n) = \sum_{i=1}^{n} x_i\cdot\lg(\frac{1}{x_i})$
\end{center}

\item 1980 G{\"u}ttler, Mehlhorn and Schneider gave new heuristic: \\
\textbf{The Modified Minimum Entropy Heuristic}

\item Greedy top-down approach:

\item Selects root $B_i$ such that $H(P_L(B_i), p_i, P_R(B_i))$ is maximized

\item $O(n^2)$ time, $O(n)$ space

\item Extremely interesting idea

\item Unfortunately, best upper bound: $C_{ME} \leq c_1\cdot H+2$ \\ where $c_1=\frac{1}{H(\frac{1}{3}, \frac{2}{3})} \approx 1.08$ 

\item We show the expected cost is $C_{ME} \leq H+4$


\end{itemize}

\end{frame}

\begin{frame} \frametitle{Approximately Optimum Binary Search Trees}

\begin{itemize}
\item In 1993 De Prisco and De Santis gave a new heuristic \cite{de1993binary}

\item Worst case expected cost: $H+1-q_0-q_n+q_{max}$ \\
 where $q_{max}$ is the maximum weight leaf node

\item Updated in 2009 by Bose and Dou\"{i}eb to: \\
$H + 1 - q_0 - q_n + q_{max} - \sum_{i=0}^{m'} pq_{\text{rank}[i]}$

\item $m'=\max({2n-3P,P})-1 \geq \frac{n}{2} - 1$ 

\item $P$ is number of increasing or decreasing sequences in a left-to-right read of the access probabilities of the leaves

\item $pq_{\text{rank}[i]}$ is the $i^{th}$ smallest access probability among all keys and leaves except $q_0$ and $q_n$.


\end{itemize}

\end{frame}

 
\begin{frame} \frametitle{Alphabetic Trees}
\begin{scriptsize}
\begin{center}
\begin{tikzpicture}[scale = 0.6]
\node[arn_small] {}
  child {node [arn_small] {}
    child {node [arn_small] {0.3 BE}}
    child {node [arn_small] {0.4 THE}}
  }  
  child {node [arn_small] {0.3 TO}
  };
\end{tikzpicture}
\end{center}
\end{scriptsize}

\begin{itemize}
\item Given $n$ keys with $\sum_{i=1}^{n} p_i = 1$

\item $n$ keys are leaves, every internal node has two children

\item We wish to minimize expected search cost: $\sum_{i=1}^{i=n} p_i \cdot d_T(B_i)$


\item Similar to Huffman code, but have an ordering over keys \cite{huffman1952method}

\end{itemize}

\end{frame}

\begin{frame}\frametitle{Alphabetic Trees}
\begin{itemize}

\item Independent optimal solutions due to Hu and Tucker \cite{hu1971optimal} and Garsia and Wachs \cite{garsia1977new}: $O(n \lg n)$ time and $O(n)$ space

\item Both went through proof simplifications \cite{knuth1973sorting, hu1973new, hu1979binary} and \cite{kingston1988new}

\item Eventually shown to be equivalent in 1982 by Hu \cite{Hu1982Book} 

\item 1991 Yeung gave $O(n)$ approximation with worst-case expected cost at most: $H + 2 - p_1-p_n$ \cite{yeung1991alphabetic}

\item In 1993 improved by De Prisco and De Santis to \cite{de1993binary}: $H+1-p_1-p_n+p_{max}$

\item Final improvement in 2009 by Bose and Dou\"{i}eb \cite{bose2009efficient}:
$H+1 -p_1-p_n-\sum_{i=0}^m p_{\text{rank}[i]} + p_{max}$


\end{itemize}
\end{frame}



\begin{frame} \frametitle{Multiway/K-ary trees} \label{sec:MWT}
\begin{tiny}
\begin{center}

\begin{tikzpicture}[scale=0.9]

\node [circle,draw] (z){2\: 10 12}
  child {node [rectangle,draw] (a) {$x<2$}    
  }  
  child {node [circle,draw] (b) {4\: \:6\: \:8}
    child {node [rectangle,draw] (a) {$2<x<4$}}
    child {node [rectangle,draw] (a) {$4<x<6$}}
    child {node [rectangle,draw] (a) {$6<x<8$}}
    child {node [rectangle,draw] (a) {$8<x<10$}}
  }  
  child {node [rectangle,draw] (c) {$10<x<12$}
  }
  child {node [rectangle,draw] (d) {$x>12$}
  };

\end{tikzpicture}

\end{center}
\end{tiny}

\begin{itemize}

\item Given $n$ keys and $n+1$ gaps with probabilities

\item Up to $k-1$ keys in internal node, single gap in a leaf node

\item Cost of search within an internal node is constant

\item Expected cost of search is still: \\
$\sum_{i=1}^{n} p_i \cdot (d_T(B_i)+1) + \sum_{j=0}^{n} q_j \cdot d_T((B_{i-1},B_i))$


\end{itemize}

\end{frame}

\begin{frame}

\begin{itemize}

\item Vishnavi et al. \cite{vaishnavi1980optimum}, and Gotlieb  \cite{gotlieb1981optimal} independently solved optimally in $O(k\cdot n^3)$ time

\item In 1997, Becker propsed an $O(Dkn)$ time solution where D is the height of the tree \cite{becker1997construction}

\item Thought, but not proven, to be close to optimal

\item Bose and Dou\"{i}eb's 2009 work also solved this problem with an $O(n)$ solution with worst case expected cost:
\begin{scriptsize}
\begin{align*}
\frac{H}{\lg(2k-1)} \leq P_{OPT} \leq P_T \leq \frac{H}{\lg k} + 1 + \sum_{i=0}^n q_i - q_0 - q_n - \sum_{i=0}^m q_{\text{rank}[i]}
\end{align*}
\end{scriptsize}

\end{itemize}


\end{frame}


\section{An Improved Bound for the Modified Minimum Entropy Heuristic}\label{An Improved Bound for the Modified Minimum Entropy Heuristic}

\begin{frame}

In this chapter we show that the Modified Minimum Entropy Heuristic of G{\"u}ttler, Mehlhorn and Schneider \cite{guttler1980binary} is within an additive factor of entropy: at worst $H+4$.
\end{frame}


\begin{frame} \frametitle{Preliminaries}

Recall \ref{1.1}, $H = \sum_{i=1}^{n} p_i\cdot\lg(\frac{1}{p_i}) + \sum_{j=0}^{n} q_i\cdot\lg(\frac{1}{q_j})$. We also use
\begin{align*}
H(x_1,x_2,...,x_n) = \sum_{i=1}^{n} x_i\cdot\lg(\frac{1}{x_i})
\end{align*} to describe the entropy of any probability distribution $(x_1, x_2, ...., x_n)$. For subtree $t$, we let 
\begin{align*}
p_t=\sum_{i : B_i \in t} p_i + \sum_{i : (B_i, B_{i+1}) \in t} q_i
\end{align*}
 be its total probability (the sum of the probability of all nodes within the subtree). $P_{L}(B_i)$ and $P_{R}(B_i)$ are probabilities of searching for a key lexicographically before or after (respectively) key $B_i$. $P_{L}(B_i, B_{i+1})$ and $P_{R}(B_i, B_{i+1})$ are probabilities of searching for a key lexicographically before (or equal to) $B_i$ and after (or equal to) $B_{i+1}$ respectively. For a subtree  $t$ $P_{L_t}(B_i)$ and $P_{R_t}(B_i)$ describe the normalized probabilities of searching for a key to the left or right of the root respectively. Formally,
\begin{align*}
P_{L_t}(B_i) = \frac{\sum_{i : B_i \in t} p_i + \sum_{i : (B_i, B_{i+1}) \in t} q_i}{p_t}.
\end{align*}
$P_{L_t}(B_i,B_{i+1})$ and $P_{R_t}(B_i,B_{i+1})$ have analogous definitions. 
We let  
\begin{equation}
E_t=H(P_{L_t}(B_i), \frac{p_i}{p_t}, P_{R_t}(B_i))
\end{equation} be the local entropy of a subtree $t$ rooted at key $B_i$.  
\end{frame}


\begin{frame} \frametitle{The Modified Entropy Rule}\label{The Modified Entropy Rule}

As described in \cite{guttler1980binary}, I first describe the entropy rule for greedy root selection then explain how it was modified. For a subtree $t$ with probability $p_t$, the entropy rule greedily chooses the key $B_i$ as the root such that $H(P_{L_t}(B_i), \frac{p_i}{p_t}, P_{R_t}(B_i))$ is maximized. While this rule behaves quite well in practice, certain cases cause it to have poor performance (refer to Figure 3.1). 


\begin{figure}[H]
\centering
% Set the overall layout of the tree
\tikzstyle{level 1}=[level distance=3cm, sibling distance=3cm]
\tikzstyle{level 2}=[level distance=3cm, sibling distance=3cm]
\scriptsize
\begin{subfigure}{.46\textwidth}
\centering
\begin{tikzpicture}[scale=0.5]
\node [circle,draw] (z){$\frac{1}{5}$}
  child {node [circle,draw] (a) {$\frac{1}{5}$}    
  }  
  child {node [circle,draw] (b) {$0$}
	  child {node [circle,draw] (e) {$0$}}
	  child {node [circle,draw] (f) {$\frac{3}{5}$}}
  };
\end{tikzpicture}
\caption{Entropy rule tree: $C=\frac{8}{5}$}
\end{subfigure}
\begin{subfigure}{.46\textwidth}
\centering
\begin{tikzpicture}[scale=0.5]
\node [circle,draw] (z){$0$}
  child {node [circle,draw] (a) {$\frac{1}{5}$} 
  	child {node [circle,draw] (e) {$\frac{1}{5}$}}
	  child {node [circle,draw] (f) {$0$}}   
  }  
  child {node [circle,draw] (b) {$\frac{3}{5}$}
  };
\end{tikzpicture}
\caption{Modified entropy rule tree: $C=\frac{7}{5}$}
\end{subfigure}
\caption{Comparison of entropy and modified entropy rule heuristics}
\end{figure}

Figure 3.1 demonstrates the shortcomings of the entropy rule heuristic. Given the probability set $\{q_0 = \frac{1}{5}, p_1 = \frac{1}{5}, q_1 = 0, p_2 = 0, q_3 = \frac{3}{5}\}$ the entropy rule will mistakenly choose key $B_1$ as the root while selecting $B_2$ as the root produces a better tree. This mistake is remedied in the modified entropy rule of G{\"u}ttler, Mehlhorn and Schneider \cite{guttler1980binary}. The modified entropy heuristic chooses the root in of the following three ways:


If there exists key $B_i$ such that $\frac{p_i}{p_t} > \max(P_{L_t}(p_i), P_{R_t}(p_i))$ we always select $B_i$ as the root.

If there exists a gap $(B_i, B_{i+1})$ such that $\frac{q_i}{p_t} > max(P_{L_t}(B_i, B_{i+1}), P_{R_t}(B_i, B_{i+1}))$ then we select the root from among $B_i$ and $B_{i+1}$. $B_i$ is chosen if $P_{L_t}(B_i, B_{i+1}) > P_{R_t}(B_i, B_{i+1})$ and $B_{i+1}$ is chosen otherwise.

Otherwise, $B_i$ is selected such that $H(P_{L_t}(B_i), \frac{p_i}{p_t}, P_{R_t}(B_i))$ is maximized (as in the original entropy rule).



The approach proposed by G{\"u}ttler, Mehlhorn and Schneider takes $O(n^2)$ time in the worst case and $O(n)$ space.
\end{frame}


\begin{frame} \frametitle{Modified Entropy is Within 4 of Entropy}

First, we review a quick Lemma about entropy (nearly identical to Lemma 2.3 in \cite{bayer1975improved}).

\begin{lem}\label{entr2x}
If $x \leq \frac{1}{2}$ then $H(x, 1-x) \geq 2x$.
\end{lem}
\begin{proof}
We refer the reader to Gallager's 1968 work \cite{gallager1968information}.
\end{proof}

Next, we describe a Lemma which breaks our choice of root in the greedy modified entropy heuristic into one of three cases (not to be confused with the three \textit{rules} used in~\ref{The Modified Entropy Rule}). 

\begin{lem}\label{MECases}
When using the ME Rule to choose the root $B_r$ of a binary search tree $t$ with total probability $p_t$, one of the following three cases must occur: \\
\textit{Case 1)} $E_t \geq 1-2 \frac{p_r}{p_t}$ \\
\textit{Case 2)} There exists gap $(B_i, B_{i+1})$ such that $\frac{q_i}{p_t} > max(P_{L_t}(B_i, B_{i+1}), P_{R_t}(B_i, B_{i+1}))$\\
\textit{Case 3)}  $\max(P_{L_t}(B_r), P_{R_t}(B_r)) < \frac{4}{5}$\\
\end{lem}
\begin{proof} At a high level, we first show that \textit{Rule a} from~\ref{The Modified Entropy Rule} implies \textit{Case 1}. We also show that if there exists $p_i$ such that $\frac{p_i}{p_t} > \max(P_{L_t}(B_i), P_{R_t}(B_i))$, but cannot apply \textit{Rule a}, then we still have \textit{Case 1}. Assuming that neither of the two aforementioned conditions occur, we must have that there exists some gap $(B_i, B_{i+1})$ spanning the middle of the data set. Given this condition, we show that if \textit{Case 2} does not occur (i.e. we cannot use \textit{Rule b}~\ref{The Modified Entropy Rule}) then \textit{Case 3} must occur, completing the proof. 


\noindent\textbf{\textit{Rule a) $\implies$ Case 1}} \\
 First, suppose there exists some $p_i$ such that $\frac{p_i}{p_t} > \max(P_{L_t}(B_i), P_{R_t}(B_i))$. By the \textit{Rule c)}~\ref{The Modified Entropy Rule}, it must be selected as the root and thus $r=i$. Moreover, both $P_{L_t}(B_i)$ and $P_{R_t}(B_i))$ must be less than one half. Thus, using \ref{entr2x} we have: 
\begin{align*}
E_t &\geq H( max(P_{L_t}(p_i), P_{R_t}(p_i)), 1-max(P_{L_t}(p_i), P_{R_t}(p_i)) \\
 &\geq 2\cdot \max(P_{L_t}(p_i), P_{R_t}(p_i)) \\  &\geq 1-\frac{p_i}{p_t} \\ 
 &\geq 1-2 \frac{p_i}{p_t} \geq 1-2 \frac{p_r}{p_t} 
\end{align*}
 as required. \\
 
\noindent\textbf{\textit{$B_i$ spans middle $\implies$ Case 1}} \\
If we do not have some $p_i$ such that $\frac{p_i}{p_t} > \max(P_{L_t}(B_i), P_{R_t}(B_i))$ but do have some $B_i$ such that $P_{L_t}(B_i) \leq \frac{1}{2}$ and $P_{R_t}(B_i) \leq \frac{1}{2}$) then we must use \textit{Rule c}~\ref{The Modified Entropy Rule} and we get:
\begin{align*}
 E_t &\geq H(P_{L_t}(B_i), \frac{B_i}{p_t} , P_{R_t}(B_i) \\
 0 &\leq P_{L_t}(B_i) \leq 0.5 \\
 0 &\leq \frac{p_i}{p_t} \leq 0.5 \\
 0 &\leq P_{R_t}(B_i) \leq 0.5
\end{align*} 
   then we know that \\
\begin{align*}
H(P_{L_t}(p_i), \frac{p_i}{p_t} , P_{R_t}(p_i)) \geq H(1/2, 1/2) = 1
\end{align*}
in this case.
 Thus, combining the above two cases, if have some $B_i$ such that $P_{L_t}(B_i) \leq \frac{1}{2}$ and $P_{R_t}(B_i) \leq \frac{1}{2}$ then $E_t \geq 1-2p_r$ as required. \\

\noindent\textbf{\textit{$\big($NOT($B_i$ spans middle) AND NOT(Case 2)$\big)$ $\implies$ Case 3}} \\
Otherwise, we must have some gap $(B_i, B_{i+1})$ spanning the middle of the data set (i.e. $P_{L_t}(B_i, B_{i+1}) < \frac{1}{2}$ and $P_{R_t}(B_i, B_{i+1}) < \frac{1}{2}$). Suppose that \textit{Case 2} does not occur (i.e. we cannot use \textit{Rule b}): there does not exist a $(B_i, B_{i+1})$ such that $\frac{q_i}{p_t} > max(P_{L_t}(B_i, B_{i+1}), P_{R_t}(B_i, B_{i+1}))$. Then, for any root $r$ of $t$ we have that \\
\begin{align*}
\max(P_{L_t}(B_r), P_{R_t}(B_r)) &\geq \min(P_{L_t}(B_r), P_{R_t}(B_r))\\ 
\max(P_{L_t}(B_r), P_{R_t}(B_r)) &\geq q_i
\end{align*}
Thus,
\begin{align*}
\max(P_{L_t}(p_r), P_{R_t}(p_r)) \geq 1/3
\end{align*}
 and by our assumption 
\begin{align*}
\max(P_{L_t}(p_r), P_{R_t}(p_r)) < \frac{1}{2}.
\end{align*}
 So, as in the proof of table 3 (5.3) in \cite{guttler1980binary}\\
\begin{align*}
E_t \geq H(1/3, 2/3) \approx 0.92.
\end{align*}
 Since \textit{Case 1} does not occur, we have that:
\begin{align*}
E_t &< 1-2\frac{p_r}{q_t} \\
\implies \frac{p_r}{q_t} &< \frac{1-H(\frac{1}{3}, \frac{2}{3})}{2} \approx 0.04.
\end{align*}

Suppose for contradiction that $\max(P_{L_t}(p_r), P_{R_t}(p_r)) \geq \frac{4}{5} p_t$ then we have:
\begin{align*}
E_t \leq H(\frac{4}{5}, \frac{1-H(\frac{1}{3}, \frac{2}{3})}{2}, \frac{1}{5}-\frac{1-H(\frac{1}{3}, \frac{2}{3})}{2}) \approx 0.87 < 0.92 \approx H(\frac{1}{3}, \frac{2}{3}) \leq E_t
\end{align*}
which is a contradiction.
Thus, if we do not have \textit{Case 1} or \textit{Case 2} we must have \textit{Case 3} which completes the proof.



\end{proof}

Before we examine the main theorem we show a small claim.

\newtheorem{claim}{Claim}
\begin{claim}\label{Claim1}
$H(\frac{1}{2}-\frac{1}{2} x, \frac{1}{2} + \frac{1}{2} x) \geq 1- \frac{4}{5} (x)^2$ when $0 < x < \frac{1}{2}$
\end{claim}

\begin{proof}
In order to prove the claim, we find the minimum of
\begin{align*}
H(\frac{1}{2}-\frac{1}{2} x, \frac{1}{2} + \frac{1}{2} x) - (1 - \frac{4}{5} (x)^2) 
\end{align*}
when $0 < x < \frac{1}{2}$. To do this, we define $F(x)$ and take the derivative with respect to $x$.
\begin{align*}
F(x) &= H(\frac{1}{2}-\frac{1}{2} x, \frac{1}{2} + \frac{1}{2} x) - (1 - \frac{4}{5} (x)^2) \\
F(x) &= - (\frac{1}{2}-\frac{1}{2} x)\cdot \lg(\frac{1}{2}-\frac{1}{2} x) - (\frac{1}{2} + \frac{1}{2} x)\cdot\lg(\frac{1}{2} + \frac{1}{2} x) - (1 - \frac{4}{5} (x)^2) \\
\implies F'(x) &= \lg(\frac{1}{2}-\frac{1}{2} x) - \lg(\frac{1}{2} + \frac{1}{2} x) + \frac{8}{5}x \text{ (with some careful manipulation)}
\end{align*}

The only root occurs when $x = 0$. Thus, we check when $x \rightarrow 0$ and $x \rightarrow \frac{1}{2}$. We note that: \\
$F'(x) \xrightarrow{x \to 0} 0^{+}$ and \\
$F'(x) \xrightarrow{x \to \frac{1}{2}} 0.0112781 > 0$. \\
Thus, $H(\frac{1}{2}-\frac{1}{2} x, \frac{1}{2} + \frac{1}{2} x) - (1 - \frac{4}{5} (x)^2) > 0$ for $ 0 < x < \frac{1}{2}$ which proves the claim.
\end{proof} 


\begin{thm}
$C_{ME} \leq H + 4$
\end{thm}

\begin{proof}
This uses a similar style to the proof of theorem 4.4 in \cite{bayer1975improved}.
We bind each $E_t$ for each subtree of our BST on a case by case basis using the cases of Lemma~\ref{MECases}.\\
If \textit{Case 1} occurs, we obviously have that
\begin{equation}\label{EQC1}
E_t \geq 1-2 \frac{p_r}{p_t}
\end{equation}
Note that this can only happen once for each key (a key can only be root once). \\

As mentioned in Lemma~\ref{MECases} if some $B_i$ spans in the middle of the data set, $P_{L_t}(B_i) \leq \frac{1}{2}$ and $P_{R_t}(B_i) \leq \frac{1}{2}$, we can still show that \textit{Case 1} occurs. Suppose for the remainder of the proof that there is no such middle-spanning $B_i$.

Let $(B_m, B_{m+1})$ be the unique middle gap (i.e. $P_{L_t}(B_m, B_{m+1}) < \frac{1}{2}$ and $P_{R_t}(B_m, B_{m+1}) < \frac{1}{2}$) when \textit{Case 2} or \textit{Case 3} occurs. When \textit{Case 2} occurs we have that (using Lemma~\ref{entr2x}):
\begin{equation}\label{EQC2} 
E_t \geq H(\frac{1}{2}-\frac{1}{2} \frac{q_m}{p_t}, \frac{1}{2} + \frac{1}{2} \frac{q_m}{p_t}) \geq 2(\frac{1}{2}-\frac{1}{2} \frac{q_m}{p_t})=1-\frac{q_m}{p_t}.
\end{equation}
Note that by the definition of the \textit{Rule c}~\ref{The Modified Entropy Rule}, when this occurs, $(B_m, B_{m+1})$ must be a leaf of depth at most 2. Thus, this condition can only happen twice for each $(B_m, B_{m+1})$ gap. \\

When \textit{Case 2} does not occur (and we have a $(B_m, B_{m+1})$ spanning the middle) we must have \textit{Case 3} (since we are still assuming that \textit{Case 1} does not occur). This gives us:
\begin{align*}
E_t \geq H(\frac{1}{2}-\frac{1}{2} \frac{q_m}{p_t}, \frac{1}{2} + \frac{1}{2} \frac{q_m}{p_t})
\end{align*}
We again apply Claim~\ref{Claim1} and get:
\begin{equation}\label{EQC3}
E_t \geq 1- \frac{4}{5} (\frac{q_m}{p_t})^2
\end{equation}

As in \cite{bayer1975improved} we define a $b_t$ for each subtree $t$ as follows. We want to have a value for $b_t$ such that $E_t \geq 1 - b_t$ in all cases. Using \textit{Cases 1,2}, and \textit{3} are their respective equations~\ref{EQC1},~\ref{EQC2}, and~\ref{EQC3} we do just that: \\
Let $b_t=2\cdot p_r$ when \textit{Case 1} occurs. $B_r$ is the root of $b_t$. \\
Let $b_t=2\cdot q_m$ when \textit{Case 2} occurs. $(B_m, B_{m+1})$ is middle gap of $b_t$. \\
Let $b_t=\frac{q_m^2}{p_t}$ when \textit{Case 3} occurs. $(B_m, B_{m+1})$ is middle gap of $b_t$. \\


Note that, in 1975 P. Bayer showed that the cost $C$ of our tree could be defined as (\textbf{Lemma 2.3} \cite{bayer1975improved}): \\
\begin{align*}
C = \sum_{t \in S_T} p_t
\end{align*}
and the entropy could be calculated by
\begin{align*}
H = \sum_{t \in S_T} p_t \cdot E_t
\end{align*}  
where $S_T$ is the set of all subtrees of our tree $T$. \\

\noindent Thus, by subbing in $E_t \geq 1 - b_t$ and rearranging we get: 
\begin{align*}
&H = \sum_{t \in S_T} p_t E_t \geq \sum_{t \in S_T} P_t - \sum_{t \in S_T} b_t = C - \sum_{t \in S_T} b_t \\
 \implies &C \leq H + \sum_{t \in S_T} b_t
\end{align*}
As mentioned above, \textit{Case 1} and \textit{Case 2} can only occur once and twice respectively for any potential root $B_r$ or gap $(B_m, B_{m+1})$. \textit{Case 3} however, can occur many times for a gap $(B_m, B_{m+1})$. Each time it occurs though, $\frac{q_m}{p_t}$ must decrease by a factor of $\frac{5}{4}$ since $\max(P_{L_t}(B_r), P_{R_t}(p_r)) < \frac{4}{5}$ for the root $B_r$ of the subtree by \textit{Case 3)} of Lemma~\ref{MECases}. Moreover, if $\frac{q_m}{p_t} > \frac{1}{2}$ then we will have \textit{Case 2}. Let $S_m$ be the set of all subtrees $t$ for which $(B_m, B_{m+1})$ is the middle gap and \textit{Case 3} only applies. We have that
\begin{align*}
C \leq H + \sum_{t \in S_T} b_t = H + 2 \sum\limits_{r = 1}^n p_r + 2 \sum\limits_{m = 0}^n q_m + \sum\limits_{m = 0}^n \sum\limits_{t \in S_m} \frac{4}{5}\frac{q_m^2}{p_t}
\end{align*}
 \\ 
By factoring out $q_m$ and examining only cases up to $\frac{q_m}{p_t} = \frac{1}{2}$ (since otherwise \textit{Case 2} will occur) we get:
\begin{align*}
C &\leq H + 2 + \sum\limits_{m = 0}^n (\frac{4}{5} \cdot q_m) \sum\limits_{x=0}^{\infty} \frac{1}{2} \cdot (\frac{4}{5}) ^ x \\
C &\leq H + 2 + \sum\limits_{m = 0}^n \frac{4}{5} \cdot q_m \cdot \frac{1}{2} \cdot (\frac{1}{1-\frac{4}{5}}) \text{    (geometric series)} \\
C &\leq H + 2 + \sum\limits_{m = 0}^n q_m \\
C &\leq H + 4
\end{align*}

\end{proof}
\end{frame}

\section{Approximate Binary Search in the Hierarchical Memory Model}\label{Approximate Binary Search in the Hierarchical Memory Model}
 \begin{frame}
In this chapter we examine the optimum BST problem under the Hierarchical Memory Model (HMM) model. We provide two approximate solutions, bound their worst case expected costs via entropy, and show improvement over a previous solution in the related HMM$_2$ model. 
\end{frame} 
 
 
\begin{frame} \frametitle{The Hierarchical Memory Model}\label{The Hierarchical Memory Model}

The HMM was proposed in 1987 by Aggarwal et al. as an alternative to the classic RAM model \cite{aggarwal1987model}. It was intended to better model the multiple levels of the memory hierarchy. The model has an unlimited number of registers, $R_1, R_2, ...$ each with its own location in memory (a positive integer). In the first version of the model, accessing a register at memory location $x_i$ takes $\lceil \lg(x_i) \rceil$ time. Thus, computing $f(a_1, a_2, ..., a_n)$ takes $\sum_{i=1}^{n} \lceil \lg(location(a_i)) \rceil$ time. The original paper also considered arbitrary cost functions $f(x)$. We will use the cost function as was explained in Thite's thesis \cite{thite2008optimum}. Here, $\mu (a)$ is the cost of accessing memory location $a$, an integer. We have a set of memory sizes $m_1, m_2, ..., m_l$ which are monotonically increasing. Each memory level has a finite size except $m_l$ which we assume has infinite size. Each memory level has an associated cost of access $c_1, c_2, ..., c_l$. We assume that $c_1 < c_2 < ... < c_l$. The cost of accessing a memory location $a$ is given by
\begin{equation}
\mu (a) = c_i \text{ if } \sum_{j = 1}^{i-1}m_j  < a \leq \sum_{j = 1}^{i}m_j.
\end{equation}


\noindent Thite notes that typical memory hierarchies have decreasing sizes for faster memory levels (moving \textit{up} the memory hierarchy). We make the same assumption:
\begin{align*}
m_1 < m_2 < ... < m_l.
\end{align*}
Unlike Thite, we also explicitly assume that successive memory level sizes divide one another evenly:
\begin{align*}
\forall i \in  \{1,2,...,l-1\} m_i \mid m_{i+1}.
\end{align*}
\end{frame}

\begin{frame} \frametitle{Optimum Binary Search Trees on the HMM Model}

Thite's thesis provides solutions to several problems in the HMM and the related HMM$_2$ models \cite{thite2008optimum}. He first provides an optimal solution to the following problem (known as \textbf{Problem 5} in the work):\\


\noindent \textbf{Problem 5 [Optimum BST Under HMM].}  \cite{thite2008optimum} Suppose we are given a set of $n$ ordered keys $x_1, x_2, ..., x_n$ with associated probabilities of search $p_1, p_2, ..., p_n$, as well as $n+1$ ranges $(- \infty, x_1), (x_1, x_2), ..., (x_{n-1}, x_n), (x_n, \infty)$ with associated probabilities of search $q_0, q_1, ..., q_n$. The problem is to construct a binary search tree $T$ over the set of keys and compute a memory assignment function $\phi : V (T) \rightarrow {1, 2, ..., n}$ that assigns nodes of $T$ to memory locations such that the expected cost of a search is minimized under the HMM model. \\

In order to remain consistent with the work in other chapters of this thesis, We will maintain the use of $B_1, B_2, ..., B_n$ to represent keys instead of Thite's $x_1, x_2, ..., x_n$ and gaps will be represented by $(B_0, B_1), (B_1, B_2), ..., (B_n, B_{n+1})$ where it is assumed that $B_0$ represents $- \infty$ and $B_{n+1}$ represents $\infty$.

Thite provided three separate optimum solutions to the problem described; \textbf{Parts}, \textbf{Trunks}, and \textbf{Split}. Here, $h$ is the minimum memory level such that all $n$ keys can fit on memories of height at most $h$. More specifically $h$ is defined as \\
\begin{align*}
\min(h \in \{1,..,l\} ): n \leq \sum_{i = 1}^{h}m_i
\end{align*}

\textbf{Parts}
bottom up DP alorithm
constructs optimum subtree $T^{*}_{i,j}$ for each $i,j$, $1 \leq i leq j \leq n$ using the first $j-i+1$ memory locations.
For each choice of root $B(k)$ where $k \in {i, ..., j}$ all possible memory assignments are examined (assigning to the $h$ levels of memory)
$O(\frac{2^{h-1}}{(h-1)!}\cdot n^{2h+1})$ final time claimed.

\textbf{Trunks}
Like parts, building optimal trees over larger subsets of keys.
For each range $i,j$, $1 \leq i leq j \leq n$, and for each possible root $B(k)$, they examine all $s$ such that $s \leq j-i+1$ and consider placing $s$ nodes in the first $h-1$ levels of memory, and the remaining $j-i-1-s$ nodes in memory level $h$.
$O(\frac{2^{n-m_h}\cdot (n-m_h+h)^{n-m_h}\cdot n^3}{(h-2)!})$ final time claimed. Works well when $n-m_h$ and $h$ are small (i.e. a short memory hierarchy with a very large biggest memory size, typical to a modern computer)

\textbf{Split}
Split is a top-down algorithm for solving the problem when there are $n$ levels in the memory hierarchy. $O(2^n)$ time claimed. Result is dubious as the explanation simply states that a root is simply chosen by examining each of the $2^{n-1}$ ways of partitioning memory locations between the left and right subtrees and recursing on each side. It is unclear how the potential roots are compared, and I cannot think of a naive way to select the correct root in this fashion. A proof of correctness of algorithm was omitted.

 In the following sections, we provide two approximate solutions to this problem that run in times $O(n \cdot \lg(m_1))$ and $O(n)$ and provide an upper bound on their expected search costs.  \\

Thite also considered the same problem under the related HMM$_2$ model. This model assumes there are simply two levels of memory of size $m_1$ and $m_2$ with costs of access $c_1$ and $c_2$ where $c_1 < c_2$. Thite provides an optimal solution to this problem (named \textbf{TwoLevel}) which runs in time $O(n^5)$.
TL has two phases, in the first phase, it uses an algorithm similar to that of Knuth, solving for all subtrees that will fit on a single memory level (i.e. solves for all ranges $[i,j]$ such that $j-i+1 \leq \max(m_1, m_2))$. Specifically, it creates arrays $C[i,j]$ and $R[i,j]$ where C and R are the optimal tree cost (using a uniform cost model), and root selection of optimal tree over $[i,j]$.
TL PH 2 implements algorithm parts and using DP to compute $c(i,j,n_1,n_2)$ and $r(i,j,n_1, n_2)$ which are the optimal tree and optimal tree root using the HMM$_2$ cost model using keys $[i,j]$ with $n_1$ keys in $M_1$ and $n_2$ in $M_2$. c computed using DP as follows:
$c(i,j,n_1,n_2) = \mu ( \theta ( B_k))* p_k + \min_{i \leq k \leq j, 0 < n_1^{(L)} < n} (c(i, k-1, n_1^{(L)}, n_2^{(L)}), (c(i, k-1, n_1^{(R)}, n_2^{(R)}))$


It runs in $o(n^5)$, if $m_1 \in o(n)$, and in $O(n^4)$ if $m_1 \in O(1)$. He also gives an $O(n\lg n)$ time approximate solution with an upper bounded expected search cost of $c_2(H+1)$. The solution we provide under the HMM model also gives an improvement over Thite's approximate algorithm in both running time and expected cost under the HMM$_2$ model.
\end{frame}

\begin{frame}[fragile] \frametitle{Algorithm ApproxMWPaging}\label{Algorithm ApproxMWPaging}

\begin{enumerate}
\item Create MWT using algorithm of Bose and Dou\"{i}eb with $m_1$ page size

\item Create balanced BST inside each node.

\item Connect the balanced BSTs to form a single BST

\item Pack into memory in BFS order.

\end{enumerate}

\noindent Left with a BST in our memory in $O(n\cdot\lg(m_1))$  \\
($O(n)$ if $m_1 \in O(1)$).

\end{frame}

\begin{frame} \frametitle{ApproxMWPaging Phase 1.}

\begin{tiny}
\begin{center}

\begin{tikzpicture}[scale=0.9]

\node [circle,draw] (z){2\: 10 12}
  child {node [rectangle,draw] (a) {$x<2$}    
  }  
  child {node [circle,draw] (b) {4\: \:6\: \:8}
    child {node [rectangle,draw] (a) {$2<x<4$}}
    child {node [rectangle,draw] (a) {$4<x<6$}}
    child {node [rectangle,draw] (a) {$6<x<8$}}
    child {node [rectangle,draw] (a) {$8<x<10$}}
  }  
  child {node [rectangle,draw] (c) {$10<x<12$}
  }
  child {node [rectangle,draw] (d) {$x>12$}
  };

\end{tikzpicture}

\end{center}
\end{tiny}

\end{frame}

\begin{frame} \frametitle{ApproxMWPaging Phase 2.}

% Set the overall layout of the tree
\tikzstyle{level 1}=[level distance=3cm, sibling distance=3.5cm]
\tikzstyle{level 2}=[level distance=3cm, sibling distance=3.5cm]

\tikzset{smnode/.style={level 1/.style={level distance=1cm, sibling distance=1cm}, circle, draw}}
\tikzstyle{level 1}=[level distance=3cm, sibling distance=3.5cm]
\tikzstyle{level 2}=[level distance=3cm, sibling distance=3.5cm]
\tikzstyle{level 3}=[level distance=1cm, sibling distance=1cm]

\begin{tiny}
\begin{center}

\begin{tikzpicture}[scale=0.5]

\node [circle,draw] (z){
  \tikzstyle{level 1}=[level distance=0.5cm, sibling distance=0.5cm]
	\begin{tikzpicture}
	\node [draw,circle] (zZ){10}
  	child {node [draw,circle] (Za) {2}    
  	}  
  	child {node [draw,circle] (Zb) {12}
  	};
	\end{tikzpicture}
}
  child {node [rectangle,draw] (a) {$x<2$}    
  }  
  child {node [circle,draw] (b) {
    \tikzstyle{level 2}=[level distance=0.5cm, sibling distance=0.5cm]
	\begin{tikzpicture}
	\node [draw,circle] (zZa){6}
  	child {node [draw,circle] (Zaa) {4}    
  	}  
  	child {node [draw,circle] (Zba) {8}
  	};
	\end{tikzpicture}  
  }
    child {node [rectangle,draw] (a) {$2<x<4$}}
    child {node [rectangle,draw] (a) {$4<x<6$}}
    child {node [rectangle,draw] (a) {$6<x<8$}}
    child {node [rectangle,draw] (a) {$8<x<10$}}
  }  
  child {node [rectangle,draw] (c) {$10<x<12$}
  }
  child {node [rectangle,draw] (d) {$x>12$}
  };

\end{tikzpicture}

\end{center}
\end{tiny}

\end{frame}

\begin{frame} \frametitle{ApproxMWPaging Phase 3.}

\begin{tiny}
\begin{center}
\begin{tikzpicture}[
  scale = 0.6,
  level 1/.style = {sibling distance=10cm},
  level 2/.style = {sibling distance=6cm}, 
  level 3/.style = {sibling distance=6cm},
  level 4/.style = {sibling distance=3cm}
]
\node [arn_small] {10}
  child { node [arn_small] {2} 
    child {node [rectangle,draw] {$x<2$}    
    }
    child {node [arn_small] {6}
      child {node [arn_small] {4}
        child {node [rectangle,draw]{$2<x<4$}}
        child {node [rectangle,draw]{$4<x<6$}}    
      }
      child {node [arn_small]{8}
        child {node [rectangle,draw]{$6<x<8$}}
        child {node [rectangle,draw]{$8<x<10$}}    
      }
    }   
  }
  child {node [arn_small] {12}    
    child {node [rectangle,draw] {$10<x<12$}
    }
    child {node [rectangle,draw] {$x>12$}
    }  
  };
\end{tikzpicture}
\end{center}
\end{tiny}

\end{frame}

\begin{frame} \frametitle{ApproxMWPaging Phase 4.}

\begin{tiny}
\begin{center}
    \begin{tabular}{ | l | l | l | l |}
    \hline
    Memory Location & Node & Left Child Location & Right Child Location \\ \hline
    1  & 10  & 2    & 3    \\ \hline
    2  & 2  & 4    & 5    \\ \hline
    3  & 12 & 6    & 7    \\ \hline
    4  & $x < 2$  & ---   & ---    \\ \hline
    5  & 6  & 8   & 9   \\ \hline
    6  & $10 < x < 12$ & ---   & ---   \\ \hline
    7  & $x > 12$ & ---   & ---   \\ \hline
    8  & 4  & 10 & 11 \\ \hline
    9  & 8  & 12 & 13 \\ \hline
    10 & $2 < x < 4$  & --- & --- \\ \hline
    11 & $4 < x < 6$  & --- & --- \\ \hline
    12 & $6 < x < 8$  & --- & --- \\ \hline
    13 & $8 < x < 10$ & --- & 16   \\ \hline
    \end{tabular}
\end{center}

\end{tiny}
\end{frame}


\begin{frame} \frametitle{Expected Cost ApproxMWPaging} \label{45}

First, we bound the depth of nodes in our BST $T$. The depth of a key $B_i$ (or $(B_{i-1}, B_i)$ for gaps) is defined as $d_T(B_i)$ (resp. $d_T(B_{i-1},B_i)$). Note that depth is the number of edges between a node and the root (i.e. the depth of the root is $0$). As in the work of Bose and Dou\"{i}eb, let $m=\max({n-3P,P})-1 \geq \frac{n}{4} - 1$ where $P$ is the number of increasing or decreasing sequences in a left-to-right read of the access probabilities of the leaves \cite{bose2009efficient}.

\begin{lem} \label{451}
For a key $B_i$,
\begin{align*} 
d_T(B_i) \leq \lg(\frac{1}{p_i}).
\end{align*}
For a key $(B_{i-1},B_i)$,
\begin{align*} 
d_T(B_{i-1},B_i) \leq \lg(\frac{1}{q_i}) + 2
\end{align*}
for all leaves, and
\begin{align*} 
d_T(B_{i-1},B_i) \leq \lg(\frac{1}{q_i}) + 1
\end{align*}
for at least $m$ of them (and the two extremal leaves, $(B_0, B_1)$ and $(B_n, B_{n+1})$). 
  
\end{lem}

\begin{proof}
First we note that in the tree $T'$ we build using Bose and Dou\"{i}eb's multiway tree algorithm, the maximum depth of keys (call this $d_{T'}(B_i), d_{T'}(B_{i-1},B_i)$) for a page size $m_1$ is \cite{bose2009efficient}:
\begin{align*} d_{T'}(B_i) &\leq \lfloor log_{m_1}(\frac{1}{p_i}) \rfloor \\
d_{T'}(B_{i-1},B_i) &\leq \lfloor log_{m_1}(\frac{2}{q_i}) \rfloor + 1 \text{ for all leaves, and}\\
d_{T'}(B_{i-1},B_i) &\leq \lfloor log_{m_1}(\frac{1}{q_i}) \rfloor + 1 \text{ for at least $m$ of them (and the two extremal leaves)}
 \end{align*}
As explained in the paper, these follow from Lemmas 1 and 2 of Bose and Dou\"{i}eb \cite{bose2009efficient}.

Inside a page, we make a balanced (ignoring weight) BST, so each key has a depth within a page of at most $\lfloor \lg(m_1) \rfloor$. Since our algorithm always connects the root of the BST made for a page to a key in the BST made for the page's parent, a key $B_i$ has a \textit{page depth} (the number of unique pages accessed in order to access the key) of at most the bounds on $d_{T'}(B_i)$ and $d_{T'}(B_{i-1},B_i)$ described. Since we examine at most $\lceil \lg(m_1) \rceil$ keys within any one page, (and only 1 gap in a leaf page) a key's depth is at most
\begin{align*}
d_T(B_i) &\leq \lfloor \lg(m_1) \rfloor \lfloor log_{m_1}(\frac{1}{p_i}) \rfloor \\
\implies d_T(B_i) &\leq \lg(m_1)\cdot log_{m_1}(\frac{1}{p_i}) \\
\implies d_T(B_i) &\leq \lg(\frac{1}{p_i}). \\
\end{align*}

\noindent For an unsuccessful search
\begin{align*}
d_T(B_{i-1},B_i) &\leq \lfloor \lg(m_1) \rfloor \lfloor log_{m_1}(\frac{2}{q_i}) \rfloor + 1 \\
\implies d_T(B_{i-1},B_i) &\leq \lg(m_1)\cdot log_{m_1}(\frac{2}{q_i}) + 1 \\
\implies d_T(B_{i-1},B_i) &\leq \lg(\frac{1}{q_i}) + 2 \text{ for all leaves, and} \\
\implies d_T(B_{i-1},B_i) &\leq \lg(\frac{1}{q_i}) + 1 \text{ for at least $m$ of them (and the two extremal leaves)}.
\end{align*}


\end{proof}

We now describe $W$, the cost of searching for a key located at the deepest node of tree $T$. Let $m'_j = \sum_{k \leq j} m_j$. We define $m'_0 = 0$. As defined in Thite's work, we let $h$ be the smallest $j$ such that $m'_j \geq n$. Let $D(T)$ be the height of $T$ (the depth of the deepest node in the tree).


\begin{lem} \label{452}
\begin{align*}
W \leq \sum_{i=1}^{h-1} \left(\lfloor \lg(m'_i+1) \rfloor - \lfloor \lg(m'_{i-1}+1 \rfloor  \right)\cdot c_i+ \left(D(T) - \lfloor \lg(m'_{h-1}+1) \rfloor \right)\cdot c_h
\end{align*}
\end{lem}


\begin{proof}
Consider the accessing each key along the path from the root to the deepest key in $T$. We will examine $D(T)$ keys. We access one key at depth $0$, one key at depth $1$, and so on. Because the tree is packed into memory in BFS order, a key a depth $i$ will be at memory index at most $2^i-1$. Now, consider how many levels of the binary search tree $T$ will fit in $m_1$. In order for all keys of depth $i$ (and higher) to be in $m_1$ we need:
\begin{align*}
2^i-1 \leq m_1 \implies i \leq \lg(m_1 + 1).
\end{align*}
Thus, at least $\lfloor \lg(m_1 + 1) \rfloor$ levels of $T$ will completely fit on $m_1$. Next we examine how many levels of $T$ fit on $m_j$ for $1 < j < l$. The last level to completely fit on $m_j$ or higher memories is the maximum $i$ such that:
\begin{align*}
2^{i+1}-1 \leq m'_j \implies i \leq \lg(m'_j + 1).
\end{align*}
Thus, at least $\lfloor \lg(m'_j + 1) \rfloor$ levels of $T$ fit on $m_j$ or higher levels of memory.
On our search for the deepest key in the tree, we make at least $\lfloor \lg(m_1 + 1) \rfloor$ checks for elements located at memory $m_1$. This costs a total of
\begin{align*}
\lfloor \lg(m_1 + 1) \rfloor \cdot c_1.
\end{align*}
For each memory level $m_j$ for $1 < j < l$, we make at least $\lfloor \lg(m'_j + 1) \rfloor$ checks in $m_j$ or higher memories. Of these checks, at least $\lfloor \lg(m'_{j-1} + 1) \rfloor$ are in memory levels strictly higher up in the memory hierarchy than $j$. Since $c_j > c_i$ for $j > i$, an upper bound on the cost of searching for all elements in memory level $j$ on the path from the root to the deepest element of the tree is:
\begin{align*}
(\lfloor \lg(m'_j + 1) \rfloor - \lfloor \lg(m'_{j-1} + 1) \rfloor) \cdot c_j.
\end{align*}
Finally, we can upper bound the cost of search within $m_h$ by assuming that all remaining searches take place at memory level $h$. The searches at level $h$ will cost at most:

\begin{align*}
(\lg(D(T) - \lfloor \lg(m'_{h-1}+1) \rfloor)\cdot c_h.
\end{align*}

Combining the above three equations (and summing over every memory level) gives us the total cost of searching for a key in the deepest part of the tree:

\begin{align*}
W &\leq \lfloor \lg(m_1 + 1) \rfloor \cdot c_1 + \sum_{i=2}^{h-1} \left(\lfloor \lg(m'_i+1) \rfloor - \lfloor \lg(m'_{i-1}+1) \rfloor \right)\cdot c_i+ \left(D(T) - \lfloor \lg(m'_{h-1}+1) \rfloor \right)\cdot c_h \\
W &\leq \sum_{i=1}^{h-1} \left(\lfloor \lg(m'_i+1) \rfloor - \lfloor \lg(m'_{i-1}+1) \rfloor \right)\cdot c_i+ \left(D(T) - \lfloor \lg(m'_{h-1}+1) \rfloor \right)\cdot c_h
\end{align*}



\end{proof}

This leads us to the following upper bound for $W$.

\begin{lem} \label{W<HT}
Assuming that $l \neq 1$:
\begin{align*}
W<D(T)\cdot c_h
\end{align*}
\end{lem}

\begin{proof}
We can rearrange Lemma~\ref{452} as follows:
\begin{align*}
W \leq D(T)\cdot c_h - \sum_{i=1}^{h-1} \lfloor \lg(m'_i+1) \rfloor \cdot(c_{i+1} - c_i) 
\end{align*}
Since we have that $c_i > c_{i-1}$ for all $i$, $\sum_{i=1}^{h-1} \lfloor \lg(m'_i+1) \rfloor \cdot(c_{i+1} - c_i)$ is strictly positive. This instantly gives the result desired.
\end{proof}

Note that $\frac{W}{D(T)}$ represents the average cost per memory access when accessing the deepest (and most costly) element of our tree. Since our costs of access are monotonically increasing as we move deeper in the tree, we will see that $\frac{W}{D(T)}$ can be used as an upper bound for the average cost per memory access when searching for any element of $T$.

\begin{lem}
The cost of searching for a keys $B_i$ and $(B_{i-1},B_i)$ ($C(B_i)$ and $C(B_{i-1},B_i)$ respectively) can be bounded as follows: 
\begin{align*} 
C(B_i) &\leq (\lg(\frac{1}{p_i})+1)\cdot \frac{W}{D(T)} < (\lg(\frac{1}{p_i})+1)\cdot c_h \\
C(B_{i-1},B_i) &\leq (\lg(\frac{1}{q_i})+2)\cdot \frac{W}{D(T)} < (\lg(\frac{1}{q_i})+2)\cdot c_h \text{ for all leaves, and} \\
C(B_{i-1},B_i) &\leq (\lg(\frac{1}{q_i})+1)\cdot \frac{W}{D(T)} < (\lg(\frac{1}{q_i})+1)\cdot c_h \text{ for at least $m$ (and the two extremal) leaves}.
\end{align*}

\end{lem}

\begin{proof}
From Lemma~\ref{451} we have a bound on the depth of keys $B_i$ and $(B_{i-1},B_i)$. We must do $d_T(B_i) + 1$ accesses to find a key and $d_T(B_{i-1},B_i)$ accesses to find a gap. Note that since our tree is stored in BFS order in memory, whenever we examine a key's child, it will be at a memory location of at least the same, if not higher cost (by being in the same or a deeper page). Thus, the cost of accessing the entire path from the root to a specific key can be upper bounded by multiplying the length of this path by the average cost per memory access of the most expensive (and deepest) key of the tree (this is exactly $\frac{W}{D(T)}$). Note that when searching for keys, we must search along the entire path from root to the key in question, while we need only examine the path from the root to the parent of a key for unsuccessful $(B_{i-1},B_i)$ searches. Combining~\ref{451}, ~\ref{452} and~\ref{W<HT} gives
\begin{align*}
C(B_i) &\leq (\lg(\frac{1}{p_i})+1)\cdot\frac{W}{D(T)}\\
\implies C(B_i) &< (\lg(\frac{1}{p_i})+1)\cdot\frac{D(T)\cdot c_h}{D(T)}   \\
\implies C(B_i) &< (\lg(\frac{1}{p_i})+1)\cdot c_h
\end{align*}

For leaves we have that
\begin{align*}
C(B_{i-1},B_i) &\leq (\lg(\frac{1}{q_i})+2)\cdot \frac{W}{D(T)}   \\
\implies C(B_{i-1},B_i) &\leq (\lg(\frac{1}{q_i})+2)\cdot \frac{D(T)\cdot c_h}{D(T)}   \\
\implies C(B_{i-1},B_i) &< (\lg(\frac{1}{q_i})+2)\cdot c_h \text{ for all leaves, and} \\
\implies C(B_{i-1},B_i) &< (\lg(\frac{1}{q_i})+1)\cdot c_h \text{ for at least $m$ (and the two extremal) leaves}.
\end{align*}
\end{proof}

We can now bound the expected cost of search using the bounds for the cost each key.

\begin{thm} \label{ApproxMWPagingThm}
\begin{align*}
C &\leq  \frac{W}{D(T)} \cdot  \left(H + 1 + \sum_{i=0}^n q_i - q_0 - q_n - \sum_{i=0}^m pq_{\text{rank}[i]} \right) \text{ and}\\
C &< \left(H + 1 + \sum_{i=0}^n q_i - q_0 - q_n - \sum_{i=0}^m pq_{\text{rank}[i]} \right) \cdot  c_h
\end{align*}
where $pq_{\text{rank}[i]}$ is the $i^{th}$ smallest access probability among all keys and leaves except $q_0$ and $q_n$.
\end{thm}

\begin{proof}
The total expected cost of search is simply the sum of the weighted cost of search for all keys multiplied by the probability of searching for each key. Given our last lemma, we have that:
\begin{align*}
C &\leq \sum_{i=1}^{n} p_i\cdot C(B_i) + \sum_{j=1}^{n+1} q_j\cdot C(B_{i-1},B_i) \\
\implies C &\leq \sum_{i=1}^{n} p_i\cdot (\lg(\frac{1}{p_i})+1)\cdot \frac{W}{D(T)} + \left(\sum_{i=0}^{n} q_i(\lg(\frac{1}{q_i})+2) - q_0 - q_n -  \sum_{i=0}^m q_{\text{rank}[i]} \right)\cdot \frac{W}{D(T)} \\
\implies C &\leq \frac{W}{D(T)} \left(\sum_{i=1}^{n} p_i\lg(\frac{1}{p_i}) + \sum_{i=0}^{n} q_i\lg(\frac{1}{q_i}) + \sum_{i=1}^{n} p_i + 2\sum_{i=0}^{n} q_i - q_0 - q_n - \sum_{i=0}^m q_{\text{rank}[i]} \right) \\
\implies C &\leq  \frac{W}{D(T)} \left(H + 1 + \sum_{i=0}^n q_i - q_0 - q_n - \sum_{i=0}^m q_{\text{rank}[i]} \right)
\end{align*}
By Lemma~\ref{W<HT} this gives:
\begin{align*}
C < \left(H + 1 + \sum_{i=0}^n q_i - q_0 - q_n - \sum_{i=0}^m q_{\text{rank}[i]} \right) \cdot  c_h
\end{align*}

\end{proof}
\end{frame}


\begin{frame} \frametitle{Approximate Binary Search Trees of De Prisco and De Santis with Extensions by Bose and Dou\"{i}eb} \label{sec:deBST}

We provide another approach to building the approximately optimal BST under the HMM model. This approach uses the approximate BST solution (in the simple RAM model) of De Prisco and De Santos (modified by Bose and Dou\"{i}eb) \cite{de1993binary, bose2009efficient}. we explain the method here.

As in the classic Knuth problem, we are given a set of $n$ probabilities of searching for keys ($p_1, p_2, ..., p_n$), as well as $n+1$ probabilities of unsuccessful searches ($q_0, q_1, ..., q_n$). De Prisco and De Santos give an algorithm which constructs a binary search tree in $O(n)$ time with an expected cost of at most \cite{de1993binary}
\begin{align*}
H+1-q_0-q_n+q_{max}
\end{align*}
  where $q_{max}$ is the maximum probability of an unsuccessful search. This was later modified by Bose and Dou\"{i}eb (the same paper described in section~\ref{43}) to have an improved bound of \cite{bose2009efficient}
\begin{align*}
H + 1 - q_0 - q_n + q_{max} - \sum_{i=0}^{m'} pq_{\text{rank}[i]}.
\end{align*}
Here, $P$ is the number of increasing or decreasing sequences in a left-to-right read of the access probabilities of the leaves and $m'=\max({2n-3P,P})-1 \geq \frac{n}{2} - 1$.  Moreover, $pq_{\text{rank}[i]}$ is the $i^{th}$ smallest access probability among all keys and leaves except $q_0$ and $q_n$.

First, we explain the algorithm of De Prisco and De Santis and then explain the extensions of Bose and Dou\"{i}eb. De Prisco and De Santis' algorithm occurs in three phases.

\begin{itemize}
\item[\textbf{Phase 1}] An auxiliary probability distribution is created using $2n$ zero probabilities, along with the $2n+1$ successful and unsuccessful search probabilities. Yeung's linear time alphabetic search tree algorithm is used with the $2n+1$ successful and unsuccessful search probabilities used as leaves of the new tree created \cite{yeung1991alphabetic}. This is referred to as the \textit{starting tree}.

\item[\textbf{Phase 2}] What's known as the \textit{redundant tree} is created by moving $p_i$ keys up the \textit{starting tree} to the lowest common ancestor of keys $q_{i-1}$ and $q_i$. The keys which used to be called $p_i$ are relabelled to $old.p_i$.

\item[\textbf{Phase 3}] The \textit{derived tree} is constructed from the \textit{redundant tree} by removing redundant edges. Edges to and from nodes which represented zero probability keys are deleted. This \textit{derived tree} is a binary search tree with the expected search cost described.
\end{itemize}


In Bose and Dou\"{i}eb's work, they explain how they can substitute their algorithm for Yeung's linear time alphabetic search tree algorithm which results in a better bound (as described above). We use the updated version (by Bose and Dou\"{i}eb) of De Prisco and De Santis' algorithm as a subroutine in the sections to follow.
\end{frame}


\begin{frame} \frametitle{Algorithm ApproxBSTPaging}\label{Algorithm ApproxBSTPaging}

Our second solution to create an approximately optimal BST under the HMM model works as follows: \\

\begin{enumerate}
\item First, we create a BST $T$ using the algorithm of De Prisco and De Santis \cite{de1993binary} (as updated by Bose and Dou\"{i}eb \cite{bose2009efficient}). This takes $O(n)$ time. \\

\item In a similar fashion to step 4) of \textit{ApproxMWPaging}, we pack keys from $T$ into memory in a breadth-first search order starting from the root. This relatively simple traversal also takes $O(n)$ time.
\end{enumerate}

We are left with a binary search tree which is properly packed into memory in total time $O(n)$.
\end{frame}


\begin{frame} \frametitle{Expected Cost ApproxBSTPaging}\label{48}

As explained in the Bose and Dou\"{i}eb paper, the average path length search cost of the tree created by their algorithm is at most: \cite{bose2009efficient}
\begin{align*}
H + 1 - q_0 - q_n + q_{max} - \sum_{i=0}^{m'} pq_{\text{rank}[i]}
\end{align*}

We call this value $P_T$ (the average search cost of tree $T$). Similar to the proof in section~\ref{45}, if we can bound the cost of search for a given path length, then we can form a bound on the average cost of search in the HMM model. As before, we can describe the cost of searching for a key located at deepest node of tree $T$: $W$. Recall, $m'_j = \sum_{k \leq j} m_k$, $m'_0 = 0$ and let $h$ be the smallest $j$ such that $m'_j \geq n$.

\begin{lem} 
When using the ApproxBSTPaging, the cost of searching for a node deepest in the tree is at most:
\begin{align*}
W \leq \sum_{k=1}^{h-1} \left(\lfloor \lg(m'_i+1) \rfloor - \lfloor \lg(m'_{i-1}+1) \rfloor)\cdot c_i+ (D(T) - \lfloor \lg(m'_{h-1}+1) \rfloor \right)\cdot c_h
\end{align*}
\end{lem}

\begin{proof}
Since we are simply putting keys into memory in BFS order, and all we use is the height of the tree and the memory hierarchy, the proof is identical to that of Lemma~\ref{452}.
\end{proof}

Since we have the same result as Lemma~\ref{452}, this immediately implies Lemma~\ref{W<HT} is true as well. Assuming that $l \neq 1$, we have that $W<D(T)\cdot c_h$. As in the proof of the expected cost ApproxMWPaging, $\frac{W}{D(T)}$ represents the average cost per memory access when accessing the deepest (and most costly) element of our tree. Thus, $\frac{W}{D(T)}$ upper bounds the average cost per memory access when searching for any element of $T$.
\\

\begin{thm} \label{ApproxBSTThm}
\begin{align*}
C &\leq  (\frac{W}{D(T)}) \cdot  (H + 1 - q_0 - q_n + q_{max} - \sum_{i=0}^{m'} pq_{\text{rank}[i]}) \text{ and} \\
C &<  (H + 1 - q_0 - q_n + q_{max} - \sum_{i=0}^{m'} pq_{\text{rank}[i]})\cdot c_h
\end{align*}
\end{thm}

\begin{proof}

Bose and Dou\"{i}eb show that after using their algorithm for Phase 1 of De Prisco and De Santis algorithm algorithm, every leaf of the \textit{starting tree} (all keys representing successful searches and gaps representing unsuccessful searhces) are at depth at most $\lfloor \lg(\frac{1}{p}) \rfloor + 1$  for at least $\max(2n-3P,P)-1$ of $p \in \left( \{p_1, p_2, ..., p_n \} \cup \{ q_0, q_1, ..., q_n \} \right)$ and $\lfloor \lg(\frac{1}{p}) \rfloor + 2$ for all others. Recall that $P$ is the number of peaks in the probability distribution $q_0, p_1, q_1, ..., p_n, q_n$. After phases 2 and 3 of the algorithm, each key has its depth decrease by 2, and all leaves (except one) move up the tree by one.
\begin{align*}
C &\leq \sum_{i=1}^{n} \left(p_i\cdot \frac{depth(p_i)+1}{D(T)}\cdot  W \right)+ \sum_{i=0}^{n} \left(q_i\cdot \frac{(depth(q_i))}{D(T)}\cdot  W \right) \\
\implies C &\leq \frac{W}{D(T)} \left(\sum_{i=1}^{n}(p_i\cdot (depth(p_i)+1))+ \sum_{i=0}^{n}(q_i\cdot (depth(q_i))) \right) \\
\implies C &\leq \frac{W}{D(T)} \left(\text{WeightedAveragePathLength}(T) \right) \\
\implies C &\leq  \left( \frac{W}{D(T)} \right) \cdot  \left(H + 1 - q_0 - q_n + q_{max} - \sum_{i=0}^{m'} pq_{\text{rank}[i]} \right) \text{ and by Lemma~\ref{W<HT}} \\
\implies C &<  \left(H + 1 - q_0 - q_n + q_{max} - \sum_{i=0}^{m'} pq_{\text{rank}[i]} \right)\cdot c_h
\end{align*}

\end{proof}
\end{frame}


\begin{frame} \frametitle{Improvements over Thite in the HMM$_2$ Model}

The HMM$_2$ model is the same as the general HMM model with the added constraint that there are only two types of memory (slow and fast). In Thite's thesis, he proposed both an optimal solution to the problem, as well as an approximate solution (\textit{Algorithm Approx-BST}) that runs in time $O(n \lg(n))$ \cite{thite2008optimum}. we first show that:
\begin{lem}
The proof of quality of approximation of Thite's approximate algorithm has a small mistake which raises its cost from at most: 
\begin{align*}
&c_2(H+1) \text{ to} \\
&c_2(H+1+\sum_{i=0}^{n}q_i)
\end{align*}
\end{lem}

\begin{proof}
Specifically, in Lemma 14 in 3.4.2.3 Quality of approximation in Thite's thesis, he proves that $\delta(z_k) = l+2$. Here, $\delta(z_k)$ represents the depth of a leaf node $z_k$. Note that Thite considers the depth of the root to be $1$ instead of $0$ which updates how the cost of search is calculated accordingly. $l$ represents the depth of recursion of the \textit{Approx-BST} algorithm. In Lemma 15, Thite goes on to prove that $q_k \leq 2^{-\delta(z_k)+2}$. In this proof, Thite shows that $q_k \leq 2^{-l+1}$, but makes a mistake when substituting in $l=\delta(z_k)-2$ and gets $q_k \leq 2^{-\delta(z_k)+2}$ while the correct bound is $q_k \leq 2^{-\delta(z_k)+3}$. This updated bound would change his depth bound in Lemma 16 from $\delta(z_k) \leq \lfloor \lg(\frac{1}{q_k}) \rfloor + 2$ to $\delta(z_k) \leq \lfloor \lg(\frac{1}{q_k}) \rfloor + 3$. Finally, substituting into his final equation for the upper bound on the expected cost of search for the tree would give:

\begin{align*}
&\sum_{i=1}^{n} \left(c_2 p_i \delta(B_i )+ \sum_{i=0}^{n} c_2 q_i (\delta(q_i)-1 \right)\\
\leq &\sum_{i=1}^{n} \left(c_2 p_i (\lg(\frac{1}{p_i})+1)+ \sum_{i=0}^{n} c_2 q_i (\frac{1}{q_i}-1+3 \right) \\
\leq &c_2 \cdot \left(H+1+\sum_{i=0}^{n}q_i \right)
\end{align*}
\end{proof}

This is of particular interest because if Thite's bound on \textit{Algorithm Approx-BST} had been correct, then in the case where $c_2=c_1$ (typical RAM model), Thite's method would have provided a strict improvement over the work of Bose and Dou\"{i}eb \cite{bose2009efficient} which seems unlikely since Thite used the BST approximation algorithm of Mehlhorn from 1984 \cite{mehlhorn1984sorting} (much before the work of Bose and Dou\"{i}eb).

By simply substituting in for $l=2$ we immediately get that, under this HMM$_2$ model, both ApproxMWPaging and ApproxBSTPaging provide strict improvements over Thite's \textit{Algorithm Approx-BST}. 

\begin{thm}
In the HMM$_2$ model, ApproxMWPaging has an expected cost of at most \\
\begin{align*}
C_{ApproxMWPaging} < (H + 1 + \sum_{i=0}^n q_i - q_0 - q_n - \sum_{i=0}^m q_{\text{rank}[i]}) \cdot  c_2
\end{align*}
and ApproxBSTPaging has an expected cost of at most
\begin{align*}
C_{ApproxBSTPaging} <  (H + 1 - q_0 - q_n + q_{max} - \sum_{i=0}^{m'} pq_{\text{rank}[i]})\cdot c_2.
\end{align*}
ApproxMWPaging does so in $O(n\cdot \lg(m_1))$ while ApproxBSTPaging runs in $O(n)$.
\end{thm}

\begin{proof}
We can directly sub $l=2$ into Theorems~\ref{ApproxMWPagingThm} and~\ref{ApproxBSTThm} to get the desired result (the running times are as explained in sections~\ref{Algorithm ApproxMWPaging} and~\ref{Algorithm ApproxBSTPaging}).
\end{proof}

Since both ApproxMWPaging and ApproxBSTPaging run in time $o(n\lg(n))$ (the time of Thite's \textit{Approx-BST} algorithm) and we can see that:\\
\begin{align*}
C_{ApproxMWPaging} &< (H + 1 + \sum_{i=0}^n q_i - q_0 - q_n - \sum_{i=0}^m q_{\text{rank}[i]}) \cdot  c_2 \\
&< c_2(H+1+\sum_{i=0}^{n}q_i) \\
 C_{ApproxBSTPaging} &< (H + 1 - q_0 - q_n + q_{max} - \sum_{i=0}^{m'} pq_{\text{rank}[i]})\cdot c_2 \\
 &< c_2(H+1+\sum_{i=0}^{n}q_i)
 \end{align*}

both methods provide a strictly better approximation and run faster than the \textit{Algorithm Approx-BST} of Thite.
\end{frame}


%=========================================
\section{Optimal BSTs On Unordered Multisets}\label{BST over Multisets}

\begin{frame}
In this chapter we examine a problem related to the initial optimal binary search tree problem of Knuth \cite{knuth1971optimum}. Our keys will no longer have a strict ordering and can be rearranged as we please before constructing our binary search tree. This problem is very similar to constructing optimal prefix-free binary codes which is solved using a Huffman coding. This problem differs because, (as in previous chapters) when searhing, we charge 1 for each internal node we examine but do not charge for leaf nodes (since we do not need to "examine" them).
\end{frame}


\begin{frame} \frametitle{The Multiset Binary Search Tree Problem}\label{The Multiset Binary Search Tree Problem}

 Consider a multiset (a set with possible duplicate values) of $n$ probabilities: $M = \lbag p_1, p_2, ..., p_n \rbag$ such that $\sum\limits_{i=1}^n p_i = 1$. Our goal is to create a tree $T$ which minimizes the expected path length, $P_T$, of nodes (the expected cost of our search in comparisons):
\begin{equation}
P_T = \sum_{i=1}^{n} p_i(b_i+1) - \sum_{i \in L}p_i
\end{equation}
Here, $b_i$ is the \emph{depth} of the $i$'th key of $M$ and $L$ is the set of leaves of the tree. We subtract the weight of the leaves of the tree since we need one less comparison to return a pointer a leaf node (as in the original optimal BST problem). In order to simplify our proof later we define $f_T$ (the working \emph{depth} of a node in tree $T$) as follows:
\begin{center}
\[
    f_T(p_i)= 
\begin{cases}
    b_i+1,& \text{if the } i \text{'th key is an internal node}\\
    b_i,              & \text{otherwise}.
\end{cases}
\]
\end{center}
Our expected path length can then be re-written as:
\begin{align*}
P_T = \sum_{i=1}^{n} p_i\cdot f_T(p_i).
\end{align*}
\end{frame}


\begin{frame} \frametitle{The OPT-MSBST Algorithm}\label{OPT-MSBST}

We propose the following simple algorithm entitled \textit{OPT-MSBST} for solving this multiset binary search tree problem and subsequently show that it is optimal.

\begin{enumerate}
\item First, we create a vector $R$ which is equal to the sorted (from largest to smallest) multiset $M$.

\item We create a BST $T$ as follows. The root of our tree will be $R[0]$, its two children will be $R[1]$ and $R[2]$, and so on. Formally, $R[i]$ will be placed at location $i+1$ in the BFS order of $T$.
\end{enumerate}

The complete proof of the optimality of this algorithm follows a similar form to the proof that Huffman codes are optimal \cite{huffman1952method}. First, we introduce a useful lemma.

\begin{lem}\label{MS Swap Lemma}
Let $T$ be a BST created for a multiset of probabilities as described in the problem definition in~\ref{The Multiset Binary Search Tree Problem}. Let $T'$ be the tree created by swapping the node locations of $p_j$ and $p_k$ in $T$. Then,
\begin{align*}
P_{T'}-P_T = (p_j - p_k)\cdot (f_T(p_k)-f_T(p_j)).
\end{align*}
\end{lem}

\begin{proof}
We let $f_{T'}$ represent the working height of a node in $T'$.
\begin{align*}
P_{T'}-P_T &= \sum_{i=1}^{n} p_i\cdot f_{T'}(p_i) - \sum_{i=1}^{n} p_i\cdot f_T(p_i) \\
&= p_j\cdot f_{T'}(p_j)+p_k\cdot f_{T'}(p_k) - p_j\cdot f_T(p_j) - p_k\cdot f_T(p_k)\\
&= p_j\cdot f_T(p_k)+p_k\cdot f_T(p_j) - p_j\cdot f_T(p_j) - p_k\cdot f_T(p_k)\\
&= p_j\cdot (f_T(p_k)-f_T(p_j)) + p_k \cdot (f_T(p_j) - f_T(p_k))\\
&= p_j\cdot (f_T(p_k)-f_T(p_j)) - p_k \cdot (f_T(p_k)-f_T(p_j))\\
&= (p_j - p_k)\cdot (f_T(p_k)-f_T(p_j))
\end{align*}

\end{proof}

Next, we prove the optimality of this algorithm. 

\begin{lem}\label{MSSolvesOpt}
The tree $T$ created by \textit{OPT-MSBST} for the multiset of probabilities $p$ solves the multiset binary search tree problem optimally.
\end{lem}

\begin{proof}
We prove this by inducting on $n = |M|$, the size of our multiset of probabilities. Consider the case where $n = 1$. In this case, we create the only tree possible which is obviously optimal. Suppose all trees created by \textit{OPT-MSBST} with $k-1$ nodes are optimal. Consider the case where $n=k$. Let $p_{min} \in M$ be the probability of the node placed in the final position in BFS order in $T$. By the definition of \textit{OPT-MSBST}, $p_{min}=\min_{p_i \in M}$.

\noindent We define $M'$, a multiset of probabilities of size $k-1$ with probabilities $\lbag p'_1, p'_2, ..., p'_{k-1} \rbag$ such that
\begin{align*}
p'_i=\frac{p_i}{1-p_{min}} \implies p_i = p'_i\cdot (1-p_{min}).
\end{align*}

\noindent Note that
\begin{align*}
\sum\limits_{p'_i \in M'} &= \sum_{i : p_i \in (M - \{p_{min}\})}p'_i\\
 &= \sum_{i : p_i \in (M - \{p_{min}\})}\frac{p_i}{1-p_{min}}\\
 &= \frac{1}{1-p_{min}}\cdot \sum_{i : p_i \in (M - \{p_{min}\})}p_i\\
 &= \frac{1}{1-p_{min}}\cdot (1-p_{min})\\
&= 1.
\end{align*}

Thus, let $T'$ be the tree created for $M'$ using \textit{OPT-MSBST}. By our induction hypothesis, it is optimal. Consider using \textit{OPT-MSBST} to create $T$ for $M$. During the running of the algorithm, we assume  we use an arbitrary but fixed tie-breaking rule during sorting. Because of this rule, and by the definition of \textit{OPT-MSBST}, for all $p'_i \in M'$, $f_{T'}(p'_i)=f_T(p_i)$. Now, we consider the cost of our tree $T$:
\begin{align*}
P_T &= \sum_{i:p_i \in (M - \{p_{min}\})} p_i\cdot f_T(p_i) + p_{min}\cdot f_T(p_{min}) \\
&= \sum_{i:p_i \in (M - \{p_{min}\})} p'_i\cdot (1-p_{min})\cdot f_{T'}(p_i) + p_{min}\cdot f_T(p_{min}) \\
&= (1-p_{min})\cdot \sum_{p'_i \in M'} p'_i\cdot f_{T'}(p_i) + p_{min}\cdot f_T(p_{min}) \\
&= (1-p_{min})\cdot P_{T'} + p_{min}\cdot f_T(p_{min})
\end{align*}



Suppose for contradiction that $T$ is not optimal, and there exists a better tree $Z$ (which is optimal). We let $f_Z$ represent the working depth of a node in $Z$. Note that there must exist an optimal tree $Z$ with a $p_i \in M$ such that $f_Z(p_i)$ is maximized over all $p_i \in M$, and $p_i = min_{p_j \in M}$. Otherwise, by Lemma~\ref{MS Swap Lemma}, we can swap the locations of the nodes $p_i$ (the smallest probability) and $p_j$ (the deepest node in the tree) and get a tree at least as good (if not better). We consider such a tree $Z$ and let $p_{min}$ be its minimum probability (which has maximum $f_Z$ value).

We let $Z'$ be tree made from $Z$ by removing $p_{min}$ from the tree over the probability distribution $M'$ as in the proof of Lemma~\ref{MSSolvesOpt}. As before,
\begin{align*}
p'_i=\frac{p_i}{1-p_{min}} \implies p_i = p'_i\cdot (1-p_{min}).
\end{align*}
We now describe how we remove $p_{min}$ to make $Z'$. If $p_{min}$ is a leaf, we simply take out the leaf. If it is an internal node (it must be the parent of a leaf since it has maximum $f$ value) we simply move one of its children (call it $l$), put it in $p_{min}$'s place, and make its possible other child (call it $r$) the child of $l$. Consider the cost of $Z$:
\begin{align*}
P_Z &= \sum_{p_i \in M} p_i\cdot f_Z(p_i) \\
&= \sum_{i:p_i \in (M - \{p_{min}\})} p_i\cdot f_Z(p_i) + p_{min}\cdot f_z(p_{min}) \text{ as in~\ref{MSSolvesOpt}, we have that:} \\
&= \sum_{p_i \in M'} (1-p_{min})\cdot p'_i\cdot f_Z(p_i) + p_{min}\cdot f_Z(p_{min}) \\
&= (1-p_{min})\cdot P_{Z'} + p_{min}\cdot f_Z(p_{min}) \\
\end{align*}

Note that our algorithm creates a balanced binary search tree, so the maximum value of $f_T$ over $M$ for $T$ cannot be greater than the maximum value of $f_Z$ for $Z$. Since we know that $p_{min}$ has has the maximum value for $f_T$ and for $f_Z$,  we have that $f_T(p_{min}) \leq f_Z(p_{min})$. Thus, we have
\begin{align*}
P_Z &< P_T \\
\implies (1-p_{min})\cdot P_{Z'} + p_{min}\cdot f_Z(p_{min}) &< (1-p_{min})\cdot P_{T'} + p_{min}\cdot f_T(p_{min}) \\
\implies P_{Z'} &< P_{T'} \\
\end{align*}
Which is a contradiction to the optimality of $T'$ over $k+1$ nodes. Thus, $T$ must be optimal as desired.


\end{proof}

We are now ready to prove our main theorem.

\begin{thm}
The tree $T$ created by \textit{OPT-MSBST} for the multiset of probabilities $p$ solves the multiset binary search tree problem optimally and is unique up to permutation of the assignment of nodes which have the same $f_T$ value and permutation of the assignment of nodes which correspond to the same probability.
\end{thm}

\begin{proof}
Since we already know that \textit{OPT-MSBST} provides an optimal solution, all that remains is to show that the tree $T$ (created for probability multiset $M$ with working node level function $f_T$) is unique up to the permutations described. Consider any optimal tree $Z$ for probability multiset $M$. Suppose their exists $p_i \in M$ and $p_j \in M$ such that $p_i < p_j$ and $f_Z(p_i) < f_Z(p_j)$. By Lemma~\ref{MS Swap Lemma} swapping $p_i$ and $p_j$ gives a strictly better tree, a contradiction. Thus, no such $p_i$ and $p_j$ must exist. This exactly means that $Z$ is identical to $T$ up to permutations between identical probabilities and within the same values of $f_T$ as required.
\end{proof}
\end{frame}


%=========================================



\section{Conclusion and Open Problems} \label{Conclusion and Open Problems}

\begin{frame} \frametitle{Conclusion}

In this work, we examined several problems related to the optimum BST problem originally proposed (and solved) by Knuth in 1971 \cite{knuth1971optimum}. In Chapter~\ref{An Improved Bound for the Modified Minimum Entropy Heuristic} we showed that the Modified Entropy Rule first proposed by  G{\"u}ttler, Mehlhorn and Schneider in 1980 had a worst case expected cost of $H+4$. This improved upon the previous best bound of $c\cdot H+2$ where $c \approx 1.08$ \cite{guttler1980binary}.

 In the next two chapters, we examined the problem under different models for external memory. In Chapter~\ref{Approximate Binary Search in the Hierarchical Memory Model} we showed that under the Hierarchical Memory Model (HMM) by Aggarwal et al. Our two algorithms ApproxMWPaging and ApproxBSTPaging solved the problem in time $O(n\cdot \lg(m_1))$ and $O(n)$ respectively \cite{aggarwal1987model}. Moreover in sections~\ref{45} and~\ref{48}, we showed the two solutions had worst case expected costs strictly less than
\begin{align*}
&(H + 1 + \sum_{i=0}^n q_i - q_0 - q_n - \sum_{i=0}^m q_{\text{rank}[i]}) \cdot  c_h \text{ and} \\
&(H + 1 - q_0 - q_n + q_{max} - \sum_{i=0}^{m'} pq_{\text{rank}[i]})\cdot c_h \text{ respectively  (theorems~\ref{ApproxMWPagingThm} and~\ref{ApproxBSTThm})}.
\end{align*}
   We concluded by showing a mistake in the Master's thesis of Thite, and subsequently proving our solutions provided an improvement over Thite's in the related HMM$_2$ model.

In Chapter~\ref{BST over Multisets}, we considered the optimum BST problem without explicit ordering on the keys. This essentially left us with a multiset of probabilities over which we attempted to build an optimum BST. In section~\ref{OPT-MSBST}, we described an algorithm, OPT-MSBST, and proved that it solved the problem optimally. We also showed the solution was unique up to certain permutations.

We have tabulated these results with novel contributions in bold.

\begin{table}[!hb]


\begin{center}
    \begin{tabular}{ | l | l | l | p{7.1cm} |}
    \hline
    Algorithm & Model & Running Time & Worst case expected cost \\ \hline
    \scriptsize Modified Minimum Entropy & \scriptsize RAM  & \scriptsize $O(n^2)$    & \scriptsize $\mathbf{C \leq H+4}$    \\ \hline
   \scriptsize \textbf{ApproxMWPaging}  & \scriptsize HMM  & \scriptsize $\mathbf{O(n\cdot lg(m_1))}$   & \scriptsize $\mathbf{C < (H + 1 + \sum_{i=0}^n q_i - q_0 - q_n - \sum_{i=0}^m q_{\text{rank}[i]}) \cdot  c_h}$    \\ \hline
    \scriptsize \textbf{ApproxBSTPaging}  & \scriptsize HMM & \scriptsize $\mathbf{O(n)}$    &  \scriptsize $\mathbf{C < (H + 1 - q_0 - q_n + q_{max} - \sum_{i=0}^{m'} pq_{\text{rank}[i]})\cdot c_h}$    \\ \hline
    \scriptsize \textbf{OPT-MSBST}  & \scriptsize RAM  & \scriptsize $\mathbf{O(n\cdot lg(n))}$    & \scriptsize ---    \\ \hline
    \end{tabular}
\end{center}

\caption{Models, running rimes, and worst case expected costs for algorithms discussed in this thesis.}
\end{table}
 
Several interested and related problems still remain open. Firstly, is $H+4$ a tight bound for the Modified Entropy Rule? We conjecture that this is not the case, and we believe the bound could be lowered to $H+2$. Moreover, while the metric used to search for the root in this heuristic is good, it is definitely not perfect. The root chosen (up to the modifications of the rule) has the maximum 3-way entropy split. However, this is not necessarily the best root, as selecting larger probability keys as the root can decrease the cost of the tree. It would be interesting to consider what the correct metric would be for correctly selecting the root, possibly in a greedy manner. Chapter~\ref{BST over Multisets} is ultimately an introduction into considerations for this problem. 
Finally, the work in Chapter~\ref{Approximate Binary Search in the Hierarchical Memory Model} can likely be extended to more recent models for external memory. 

\end{frame}

\bibliographystyle{abbrv}
\bibliography{ProjectPresentation}


\end{document}
