\documentclass[]{beamer}

\usepackage{beamerthemesplit} 
\usepackage{tikz}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{pifont}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{enumerate}
\usepackage{amsfonts}
\usepackage{url}
\usepackage{amsmath,amsfonts,amssymb,amsthm,epsfig,epstopdf,url,array}
\usepackage[]{units}
\usepackage{xcolor}
\usepackage{fancybox}
\usepackage{tikz}
\usepackage{stmaryrd}
\usepackage{caption}
\usepackage{subcaption}
\theoremstyle{plain}
\newtheorem{thm}{Theorem}[section]
\newtheorem{lem}[thm]{Lemma}
\newtheorem{prop}[thm]{Proposition}
\newtheorem*{cor}{Corollary}

\newcommand*\rfrac[2]{{#1}/{#2}}
\makeatletter
\newcommand{\rom}[1]{\romannumeral #1}

\definecolor{RazerGreen}{RGB}{71,225,12}
\definecolor{RazerGreen}{RGB}{71,225,12}
\setbeamercolor{title}{fg=black}
\setbeamercolor{frametitle}{fg=black}
\setbeamercolor{structure}{fg=RazerGreen}

\tikzset{
    current/.style={
		scale=0.8,
		fill=blue,
		minimum size=0.5pt,
        draw,
        circle,
    }
}
\tikzset{
    select/.style={
		scale=0.8,
		fill=red,
		minimum size=0.5pt,
        draw,
        circle,
    }
}
\tikzset{
    max/.style={
		scale=0.8,
		minimum size=0.5pt,
        draw,
        circle,
    }
}

\tikzset{
  treenode/.style = {align=center, inner sep=0pt, text centered,
    font=\sffamily\small},
  arn_n/.style = {treenode, circle, draw=black,
    fill=white, text width=2em},% arbre rouge noir, noeud noir
  arn_small/.style = {treenode, circle, draw=black,
    fill=white, text width=2em, font=\tiny, align=center},
  arn_small_r/.style = {treenode, rectangle, draw,
    fill=white, font=\tiny, align=center},
  arn_r/.style = {treenode, circle, red, draw=red, 
    text width=2em, very thick},% arbre rouge noir, noeud rouge
  arn_x/.style = {treenode, rectangle, draw=black,
    minimum width=2.2em, minimum height=2.5em}% arbre rouge noir, nil
}

\title{Approximately Optimum Search
Trees in External Memory Models}
\subtitle{Master of Mathematics Presentation}
\author{Oliver Grant \\
Supervisor: Ian Munro}



\begin{document} \date{}
\begin{frame}
  \titlepage
\end{frame}

%\tableofcontents

\section{Introduction}

\begin{frame} \frametitle{Binary Search Trees}

\begin{itemize}

\item \textbf{BST} - Simple structure used to store key-value pairs

\item First appeared in the late 1950s/early 1960s

\item Attributed to Windley, Booth, Colin and Hibbard \cite{windley1960trees, booth1960efficiency, hibbard1962some}

\item Ordering property over keys allows for quick searches

\end{itemize}
\end{frame}

\begin{frame} \frametitle{The Optimum Binary Search Tree Problem}

\begin{itemize}

\item Knuth proposed optimum BST problem in 1971 \cite{knuth1971optimum}

\item $n$ keys, $B_1, B_2, ..., B_n$

\item $2n+1$ frequencies, ${p_1, p_2, ..., p_n}$, ${q_0, q_1, ..., q_n}$

\item $p_i$, probability of searching for $B_1$

\item $q_i$, probability of searching for key in gap $(B_i, B_{i+1})$

\item We assume $\sum\limits_{i=1}^n p_i + \sum\limits_{i=0}^n q_i = 1$

\item Also assume w.l.o.g. $q_{i-1}+p_i+q_i \neq 0$

\item Keys must be internal nodes, gaps leaves

\item $P = \sum_{i=1}^{n} p_i \cdot (d_T(B_i)+1) + \sum_{i=0}^{n} q_i \cdot(d_T((B_i, B_{i+1})))$


\end{itemize}
\end{frame}


\begin{frame} \frametitle{Optimum BST Example}
\begin{center}
\begin{tikzpicture}[level/.style={sibling distance = 6cm/####1,
  level distance = 1.5cm}] 
\node [arn_n] {0.15 THE}
    child{ node [arn_n] {0.10 BE} 
            child{ node [arn_x] {0.10 \\ $x < BE$}} 
            	child{ node [arn_x] {0.40 \\  $BE < x < THE$}}
            }
    child{ node [arn_n] {0.10 TO}
            child{ node [arn_x] {0.01 \\ $THE < x < TO$}}
            child{ node [arn_x] {0.14 \\ $x > TO$}}
		}
		
; 
\end{tikzpicture}



{\scriptsize
\begin{align*}
P = &\sum_{i=1}^{n} p_i \cdot (d_T(B_i)+1) + \sum_{i=0}^{n} q_i \cdot(d_T((B_i, B_{i+1}))) \\
= &0.15 \cdot 1 + 0.10 \cdot 2 + 0.10 \cdot 2 + 0.10 \cdot 2 + 0.10 \cdot 2 + 0.40 \cdot 2 + 0.01 \cdot 2 + 0.14 \cdot 2 \\
= &1.85
\end{align*}
}%

\end{center}
\end{frame}

\begin{frame} \frametitle{Three-Way Branching}

\begin{center}
\begin{tikzpicture}
\node[arn_n] {0.4 a}
  child {node [arn_n] {0.3 b}    
  }  
  child {node [arn_n] {0.3 c}
  };
\end{tikzpicture}
\end{center}

\begin{itemize}



\item \definecolor{lightgrey}{rgb}{0.95,0.92,0.92} % Defines the color used for content box headers
\colorbox{lightgrey}{ \fontfamily{cmtt}\selectfont \uppercase{IF (expr) label1, label2, label3} } 

\item Control transferred to one of $3$ locations with one statement

\item Always single comparison under three-way branching

\item Minimum $1.6$ comparisons expected using $<, >, =$
\end{itemize}
\end{frame}

\iffalse

\begin{frame} \frametitle{Why Study Binary Search Trees}


\begin{itemize}
\item AVL trees self-balancing BST in 1963, maintains $O(\lg(n))$ height \cite{adelsonvelskii1963algorithm}

\item B-trees/Red-black trees by R. Bayer in 1972 (with later extensions by others) \cite{bayer1972symmetric}

\item  Allen and Munro examined a MTR heuristic \cite{allen1978self}

\item Splay trees of Sleator and Tarjan in 1985, dynamic optimality conjecture \cite{sleator1985self}

\item Tango trees in 2007 by Demaine et al., $O(\lg \lg n)$-competitive \cite{demaine2007dynamic}

\end{itemize}

\end{frame}
\fi

\begin{frame} \frametitle{Why Study Binary Search Trees}

\begin{itemize}

\item Binary space partitions in 3D graphics \cite{schumacker1969study, paterson1992optimal}

\item Binary tries in routers and IP lookup structures \cite{song2010building}

\item Programming languages like C++ \texttt{std::map} \cite{CppMap} 

\item Syntax trees during compilation \cite{louden1997compiler}

\item Many more examples

\item Theoretically interesting

\end{itemize}

\end{frame}


\begin{frame}{Overview}

\begin{itemize}

\item Review work on BSTs, multiway trees, and alphabetic trees

\item We have three separate results:
\end{itemize}

\begin{enumerate}
\item In the standard problem, we improve bound on MME Heuristic of G{\"u}ttler, Mehlhorn and Schneider \cite{guttler1980binary}

\item Under Hierarchical Memory Model \cite{aggarwal1987model}, two $O(n)$ time approximation algorithms

\item Given unordered probabilities show $O(n)$ algorithm within $\frac{n+1}{2n}$ of optimal
\end{enumerate}




\end{frame}

\section{Background and Related Work} \label{Background and Related Work}

\begin{frame} \frametitle{Optimum Binary Search Trees}

\begin{itemize}
\item In 1971 C. Gotlieb and Walker approximate solution \cite{walker1971top}

\item Knuth $O(n^2)$ time and space solution \cite{knuth1971optimum}

\item Approximate solutions followed

\item Interestingly, it is hard to bound the cost of approximation algorithms to the optimal

\item Discuss in terms of \textit{entropy}:
\end{itemize}

\begin{align*}
H = \sum_{i=1}^{n} p_i\cdot\lg(\frac{1}{p_i}) + \sum_{j=0}^{n} q_i\cdot\lg(\frac{1}{q_j})
\end{align*}

\end{frame}

\begin{frame} \frametitle{Approximately Optimum Binary Search Trees}


\begin{itemize}
\item In 1975, P. Bayer showed \cite{bayer1975improved}:
\begin{align*}
H-\lg H-(\lg e-1) \leq C_{Opt} \leq C_{WB}, C_{MM} \leq H + 2
\end{align*}

\item $C_{Opt}$ optimal solution

\item $C_{WB}, C_{MM}$ both top-down greedy approaches running in $O(n)$

\item $P_L(B_i)$,  $P_R(B_i)$ probabilities of searching for key before or after $B_i$ respectively

\item $C_{WB}$ weight-balanced heuristic: \\
Select root $B_i$ as root with minimum $|P_L(B_i)-P_R(B_i)|$

\item $C_{MM}$ min-max heuristic:  \\
Select $B_i$ as root with minimum $\max(P_L(B_i), P_R(B_i))$


\end{itemize}

\end{frame}

\begin{frame} \frametitle{Approximately Optimum Binary Search Trees}

\begin{itemize}

\item In addition to using $H$ to represent the entropy of our whole distribution:
\begin{center}
$H(x_1,x_2,...,x_n) = \sum_{i=1}^{n} x_i\cdot\lg(\frac{1}{x_i})$
\end{center}

\item 1980 G{\"u}ttler, Mehlhorn and Schneider gave new heuristic: \\
\textbf{The Modified Minimum Entropy Heuristic}

\item Greedy top-down approach (basic idea):

\item Selects root $B_i$ such that $H(P_L(B_i), p_i, P_R(B_i))$ is maximized

\item $O(n^2)$ time, $O(n)$ space

\item Unfortunately, best upper bound: $C_{ME} \leq c_1\cdot H+2$ \\ where $c_1=\frac{1}{H(\frac{1}{3}, \frac{2}{3})} \approx 1.08$ 

\item We show the expected cost is $C_{ME} \leq H+4$


\end{itemize}

\end{frame}

\begin{frame} \frametitle{Approximately Optimum Binary Search Trees}

\begin{itemize}
\item In 1993 De Prisco and De Santis gave a new heuristic \cite{de1993binary}

\item Worst case expected cost: $H+1-q_0-q_n+q_{max}$ \\
 where $q_{max}$ is the maximum weight leaf node

\item Updated in 2009 by Bose and Dou\"{i}eb to: \\
$H + 1 - q_0 - q_n + q_{max} - \sum_{i=0}^{m'} pq_{\text{rank}[i]}$

\item $m'=\max({2n-3P,P})-1 \geq \frac{n}{2} - 1$ 

\item $P$ is number of increasing or decreasing sequences in a left-to-right read of the access probabilities of the leaves

\item $pq_{\text{rank}[i]}$ is the $i^{th}$ smallest access probability among all keys and leaves except $q_0$ and $q_n$.


\end{itemize}

\end{frame}

 
\begin{frame} \frametitle{Alphabetic Trees}
\begin{scriptsize}
\begin{center}
\begin{tikzpicture}[scale = 0.6]
\node[arn_small] {}
  child {node [arn_small] {}
    child {node [arn_small] {0.3 BE}}
    child {node [arn_small] {0.4 THE}}
  }  
  child {node [arn_small] {0.3 TO}
  };
\end{tikzpicture}
\end{center}
\end{scriptsize}

\begin{itemize}
\item Given $n$ keys with $\sum_{i=1}^{n} p_i = 1$

\item $n$ keys are leaves, every internal node has two children

\item We wish to minimize expected search cost: $\sum_{i=1}^{i=n} p_i \cdot d_T(B_i)$


\item Similar to Huffman code, but have an ordering over keys \cite{huffman1952method}

\end{itemize}

\end{frame}

\begin{frame}\frametitle{Alphabetic Trees}
\begin{itemize}

\item Independent optimal solutions due to Hu and Tucker \cite{hu1971optimal} and Garsia and Wachs \cite{garsia1977new}: $O(n \lg n)$ time and $O(n)$ space

\item Both went through proof simplifications \cite{knuth1973sorting, hu1973new, hu1979binary} and \cite{kingston1988new}

\item Eventually shown to be equivalent in 1982 by Hu \cite{Hu1982Book} 

\item 1991 Yeung gave $O(n)$ approximation with worst-case expected cost at most: $H + 2 - p_1-p_n$ \cite{yeung1991alphabetic}

\item In 1993 improved by De Prisco and De Santis to \cite{de1993binary}: $H+1-p_1-p_n+p_{max}$

\item Final improvement in 2009 by Bose and Dou\"{i}eb \cite{bose2009efficient}:
$H+1 -p_1-p_n-\sum_{i=0}^m p_{\text{rank}[i]} + p_{max}$


\end{itemize}
\end{frame}



\begin{frame} \frametitle{Multiway/K-ary trees} \label{sec:MWT}
\begin{tiny}
\begin{center}

\begin{tikzpicture}[scale=0.9]

\node [circle,draw] (z){2\: 10 12}
  child {node [rectangle,draw] (a) {$x<2$}    
  }  
  child {node [circle,draw] (b) {4\: \:6\: \:8}
    child {node [rectangle,draw] (a) {$2<x<4$}}
    child {node [rectangle,draw] (a) {$4<x<6$}}
    child {node [rectangle,draw] (a) {$6<x<8$}}
    child {node [rectangle,draw] (a) {$8<x<10$}}
  }  
  child {node [rectangle,draw] (c) {$10<x<12$}
  }
  child {node [rectangle,draw] (d) {$x>12$}
  };

\end{tikzpicture}

\end{center}
\end{tiny}

\begin{itemize}

\item Given $n$ keys and $n+1$ gaps with probabilities

\item Up to $k-1$ keys in internal node, single gap in a leaf node

\item Cost of search within an internal node is constant

\item Expected cost of search is still: \\
$\sum_{i=1}^{n} p_i \cdot (d_T(B_i)+1) + \sum_{j=0}^{n} q_j \cdot d_T((B_{i-1},B_i))$


\end{itemize}

\end{frame}

\begin{frame}

\begin{itemize}

\item Vishnavi et al. \cite{vaishnavi1980optimum}, and Gotlieb  \cite{gotlieb1981optimal} independently solved optimally in $O(k\cdot n^3)$ time

\item In 1997, Becker propsed an $O(Dkn)$ time solution where D is the height of the tree \cite{becker1997construction}

\item Thought, but not proven, to be close to optimal

\item Bose and Dou\"{i}eb's 2009 work also solved this problem with an $O(n)$ solution with worst case expected cost:
\begin{scriptsize}
\begin{align*}
\frac{H}{\lg(2k-1)} \leq P_{OPT} \leq P_T \leq \frac{H}{\lg k} + 1 + \sum_{i=0}^n q_i - q_0 - q_n - \sum_{i=0}^m q_{\text{rank}[i]}
\end{align*}
\end{scriptsize}

\end{itemize}


\end{frame}


\section{An Improved Bound for the Modified Minimum Entropy Heuristic}\label{An Improved Bound for the Modified Minimum Entropy Heuristic}

\begin{frame} \frametitle{An Improved Bound for the Modified Minimum Entropy Heuristic}

\begin{itemize}

\item Modified Minimum Entropy (ME) Heuristic of G{\"u}ttler, Mehlhorn and Schneider \cite{guttler1980binary}

\item Previous bound: $C_{ME} \leq c_1\cdot H+2$ \\ where $c_1=\frac{1}{H(\frac{1}{3}, \frac{2}{3})} \approx 1.08$ 

\item We show the expected cost is $C_{ME} \leq H+4$

\end{itemize}

\end{frame}


\begin{frame} \frametitle{Preliminaries}

\begin{itemize}

\item For subtree $t$, we let 
\begin{equation}
p_t=\sum_{i : B_i \in t} p_i + \sum_{i : (B_i, B_{i+1}) \in t} q_i
\end{equation}
\item $P_{L_t}(B_i)$, $P_{R_t}(B_i)$: normalized probabilities of searching before or after $B_i$:
\begin{equation}
P_{L_t}(B_i) = \frac{\sum_{i : B_i \in t} p_i + \sum_{i : (B_i, B_{i+1}) \in t} q_i}{p_t}.
\end{equation}
\item $P_{L_t}(B_i,B_{i+1})$ and $P_{R_t}(B_i,B_{i+1})$ analogous. \\
\item The local entropy of subtree $t$ is:
\begin{equation}
E_t=H(P_{L_t}(B_i), \frac{p_i}{p_t}, P_{R_t}(B_i))
\end{equation} 

\end{itemize}
\end{frame}


\begin{frame} \frametitle{The Modified Entropy Rule}\label{The Entropy Rule}

\begin{itemize}
\item The basic version of the hueristic greedily chooses root in a top-down manner:
\item \begin{center}
\textit{$B_i$ is selected such that $H(P_{L_t}(B_i), \frac{p_i}{p_t}, P_{R_t}(B_i))$ is maximized (as in the original entropy rule).}
\end{center}
\end{itemize}

\end{frame}

\begin{frame} \frametitle{The Entropy Rule's Shortcomings}

\begin{figure}[H]
\centering
% Set the overall layout of the tree
\tikzstyle{level 1}=[level distance=3cm, sibling distance=3cm]
\tikzstyle{level 2}=[level distance=3cm, sibling distance=3cm]
\scriptsize
\begin{subfigure}{.46\textwidth}
\centering
\begin{tikzpicture}[scale=0.5]
\node [circle,draw] (z){$\frac{1}{5}$}
  child {node [circle,draw] (a) {$\frac{1}{5}$}    
  }  
  child {node [circle,draw] (b) {$0$}
	  child {node [circle,draw] (e) {$0$}}
	  child {node [circle,draw] (f) {$\frac{3}{5}$}}
  };
\end{tikzpicture}
\caption{Entropy rule tree: $C=\frac{8}{5}$}
\end{subfigure}
\begin{subfigure}{.46\textwidth}
\centering
\begin{tikzpicture}[scale=0.5]
\node [circle,draw] (z){$0$}
  child {node [circle,draw] (a) {$\frac{1}{5}$} 
  	child {node [circle,draw] (e) {$\frac{1}{5}$}}
	  child {node [circle,draw] (f) {$0$}}   
  }  
  child {node [circle,draw] (b) {$\frac{3}{5}$}
  };
\end{tikzpicture}
\caption{Modified entropy rule tree: $C=\frac{7}{5}$}
\end{subfigure}
\caption{Comparison of entropy and modified entropy rule heuristics}
\end{figure}

Given the probability set $\{q_0 = \frac{1}{5}, p_1 = \frac{1}{5}, q_1 = 0, p_2 = 0, q_3 = \frac{3}{5}\}$ the entropy rule will mistakenly choose key $B_1$ as the root while selecting $B_2$ as the root produces a better tree.

\end{frame}

\begin{frame} \frametitle{The Modified Minimum Entropy Rule}

\begin{itemize}
\item[\textit{a)}] If there exists key $B_i$ such that $\frac{p_i}{p_t} > \max(P_{L_t}(B_i), P_{R_t}(B_i))$ we always select $B_i$ as the root.

\item[\textit{b)}] If there exists a gap $(B_i, B_{i+1})$ such that $\frac{q_i}{p_t} > max(P_{L_t}(B_i, B_{i+1}), P_{R_t}(B_i, B_{i+1}))$ then we select the root from among $B_i$ and $B_{i+1}$. $B_i$ is chosen if $P_{L_t}(B_i, B_{i+1}) > P_{R_t}(B_i, B_{i+1})$ and $B_{i+1}$ is chosen otherwise.

\item[\textit{c)}] Otherwise, $B_i$ is selected such that $H(P_{L_t}(B_i), \frac{p_i}{p_t}, P_{R_t}(B_i))$ is maximized (as in the original entropy rule).

\end{itemize}
The approach proposed by G{\"u}ttler, Mehlhorn and Schneider takes $O(n^2)$ time in the worst case and $O(n)$ space.
\end{frame}


\begin{frame} \frametitle{Modified Entropy is Within 4 of Entropy}

\begin{itemize}
\item We start with two important equations:
\end{itemize}
\begin{align*}
C_{ME} &= \sum_{t \in S_T} p_t
 &H = \sum_{t \in S_T} p_t \cdot E_t
\end{align*}  
where $S_T$ is the set of all subtrees of our tree $T$. \\

\noindent For each subtree, we bind $E_t \geq 1 - \frac{b_t}{p_t}$ (we define $b_t$ later): 
\begin{align*}
&H = \sum_{t \in S_T} p_t E_t \geq \sum_{t \in S_T} p_t \cdot (1 - \frac{b_t}{p_t}) = C - \sum_{t \in S_T} b_t \\
 \implies &C \leq H + \sum_{t \in S_T} b_t
\end{align*}

\end{frame}

\begin{frame}

\begin{itemize}
\item For each root, we show that one of three cases must occur:

\item[\textit{Case 1)}] $E_t \geq 1-2 \frac{p_r}{p_t}$. \textbf{We set $b_t = 2p_r$.}

\item[\textit{Case 2)}] There exists gap $(B_i, B_{i+1})$ such that $\frac{q_i}{p_t} > max(P_{L_t}(B_i, B_{i+1}), P_{R_t}(B_i, B_{i+1}))$. \textbf{We set $b_t = q_i$.}

\item[\textit{Case 3)}]  $\max(P_{L_t}(B_r), P_{R_t}(B_r)) < \frac{4}{5}$. \textbf{We set $b_t = q_m$ where $q_m$ is the middle gap.}

\end{itemize}


\end{frame}

\begin{frame}

\textit{Case 1)} $E_t \geq 1-2 \frac{p_r}{p_t}$. \textbf{We set $b_t = 2p_r$.}


\begin{itemize}
\item "The easy case"
\item If we have a key in the middle this will follow.
\item Happens at most once per key.
\end{itemize}

\end{frame}
\begin{frame}
\textit{Case 2)} There exists gap $(B_i, B_{i+1})$ such that $\frac{q_i}{p_t} > max(P_{L_t}(B_i, B_{i+1}), P_{R_t}(B_i, B_{i+1}))$. \textbf{We set $b_t = q_i$.}
\begin{itemize}
\item Still fairly easy
\item If we have a big gap in the middle of our dataset, we use rule b).
\item Can happen at most twice for a gap.
\end{itemize}
\end{frame}

\begin{frame}
\textit{Case 3)} $\max(P_{L_t}(B_r), P_{R_t}(B_r)) < \frac{4}{5}$. \textbf{We set $b_t = q_m$ where $q_m$ is the middle gap.}

\begin{itemize}
\item The hard case
\item When this happens, since $\max(P_{L_t}(B_r), P_{R_t}(B_r)) < \frac{4}{5}$, our middle gap must get bigger relative to the remaining probability.
\item once it is big enough ($\geq \frac{1}{2}$), case 2 occurs.
\item We can show that $E_t \geq 1 - \frac{4}{5}\frac{q_m}{p_t}^2$ so we set $b_t = \frac{4}{5}\frac{q_m^2}{p_t}$ 
\item Each gap can contribute at most:
\begin{align*}
q_m \cdot \sum\limits_{x=0}^{\infty} \frac{1}{2} \cdot (\frac{4}{5}) ^ x = \mathbf{q_m \cdot 2}
\end{align*}
\end{itemize}

\end{frame}

\begin{frame}

\begin{itemize}
\item Combining these three cases we get a total of:
\end{itemize}

\begin{align*}
C \leq H + \sum_{t \in S_T} b_t = H + 2 \sum\limits_{r = 1}^n p_r + 2 \sum\limits_{m = 0}^n q_m + \sum\limits_{m = 0}^n \sum\limits_{t \in S_m} \frac{4}{5}\frac{q_m^2}{p_t}
C &\leq H + 2 + \sum\limits_{m = 0}^n (\frac{4}{5} \cdot q_m) \sum\limits_{x=0}^{\infty} \frac{1}{2} \cdot (\frac{4}{5}) ^ x \\
C &\leq H + 2 + 2
C &\leq H + 4
\end{align*}
\end{frame}

\section{Approximate Binary Search in the Hierarchical Memory Model}\label{Approximate Binary Search in the Hierarchical Memory Model} 
 
\begin{frame} \frametitle{The Hierarchical Memory Model}\label{The Hierarchical Memory Model}

\begin{itemize}

\item HMM by Aggarwal et al. in 1987
\item While not perfect, improvement over the RAM model
\item Hierarchy of different memories, increasing in size and decreasing in access cost
\item INSERT PICTURE

\end{itemize}
\end{frame}

\begin{frame}
\begin{itemize}
\item Formally, $R_1, R_2, ...$ unlimited number of registers each with location in memory.
\item Set of memories $M_1, M_2, ...M_l$
\item With memory sizes $m_1, m_2, ..., m_l$
\item Cost of accessing an item in $M_i$ is $c_i$
\item We assume $c_1 < c_2 < ... < c_l$
\item Cost of accessing item at location $a$:
\begin{align*}
\mu (a) = c_i \text{ if } \sum_{j = 1}^{i-1}m_j  < a \leq \sum_{j = 1}^{i}m_j.
\end{align*}
\end{itemize}
\end{frame}

\begin{frame} \frametitle{Optimum Binary Search Trees on the HMM Model}
\begin{itemize}
\item Given $n$ keys and $n+1$ gaps with associated probabilities as before.  
\item Construct binary tree $T$ (as before).
\item Put assign the nodes in $T$ to memory locations
\item Minimize the expected cost of search:
\end{itemize}
\end{frame}

\begin{frame}
TODO INSERT COST OF ACCESS PICTURE WHICH IS SUPER NEEDED


\end{frame}

\begin{frame}[fragile] \frametitle{Algorithm ApproxMWPaging}\label{Algorithm ApproxMWPaging}

\begin{enumerate}
\item Create MWT using algorithm of Bose and Dou\"{i}eb with $m_1$ page size

\item Create balanced BST inside each node.

\item Connect the balanced BSTs to form a single BST

\item Pack into memory in BFS order.

\end{enumerate}

\noindent Left with a BST in our memory in $O(n\cdot\lg(m_1))$  \\
($O(n)$ if $m_1 \in O(1)$).

\end{frame}

\begin{frame} \frametitle{ApproxMWPaging Phase 1.}

\begin{tiny}
\begin{center}

\begin{tikzpicture}[scale=0.9]

\node [circle,draw] (z){2\: 10 12}
  child {node [rectangle,draw] (a) {$x<2$}    
  }  
  child {node [circle,draw] (b) {4\: \:6\: \:8}
    child {node [rectangle,draw] (a) {$2<x<4$}}
    child {node [rectangle,draw] (a) {$4<x<6$}}
    child {node [rectangle,draw] (a) {$6<x<8$}}
    child {node [rectangle,draw] (a) {$8<x<10$}}
  }  
  child {node [rectangle,draw] (c) {$10<x<12$}
  }
  child {node [rectangle,draw] (d) {$x>12$}
  };

\end{tikzpicture}

\end{center}
\end{tiny}

\end{frame}

\begin{frame} \frametitle{ApproxMWPaging Phase 2.}

% Set the overall layout of the tree
\tikzstyle{level 1}=[level distance=3cm, sibling distance=3.5cm]
\tikzstyle{level 2}=[level distance=3cm, sibling distance=3.5cm]

\tikzset{smnode/.style={level 1/.style={level distance=1cm, sibling distance=1cm}, circle, draw}}
\tikzstyle{level 1}=[level distance=3cm, sibling distance=3.5cm]
\tikzstyle{level 2}=[level distance=3cm, sibling distance=3.5cm]
\tikzstyle{level 3}=[level distance=1cm, sibling distance=1cm]

\begin{tiny}
\begin{center}

\begin{tikzpicture}[scale=0.5]

\node [circle,draw] (z){
  \tikzstyle{level 1}=[level distance=0.5cm, sibling distance=0.5cm]
	\begin{tikzpicture}
	\node [draw,circle] (zZ){10}
  	child {node [draw,circle] (Za) {2}    
  	}  
  	child {node [draw,circle] (Zb) {12}
  	};
	\end{tikzpicture}
}
  child {node [rectangle,draw] (a) {$x<2$}    
  }  
  child {node [circle,draw] (b) {
    \tikzstyle{level 2}=[level distance=0.5cm, sibling distance=0.5cm]
	\begin{tikzpicture}
	\node [draw,circle] (zZa){6}
  	child {node [draw,circle] (Zaa) {4}    
  	}  
  	child {node [draw,circle] (Zba) {8}
  	};
	\end{tikzpicture}  
  }
    child {node [rectangle,draw] (a) {$2<x<4$}}
    child {node [rectangle,draw] (a) {$4<x<6$}}
    child {node [rectangle,draw] (a) {$6<x<8$}}
    child {node [rectangle,draw] (a) {$8<x<10$}}
  }  
  child {node [rectangle,draw] (c) {$10<x<12$}
  }
  child {node [rectangle,draw] (d) {$x>12$}
  };

\end{tikzpicture}

\end{center}
\end{tiny}

\end{frame}

\begin{frame} \frametitle{ApproxMWPaging Phase 3.}

\begin{tiny}
\begin{center}
\begin{tikzpicture}[
  scale = 0.6,
  level 1/.style = {sibling distance=10cm},
  level 2/.style = {sibling distance=6cm}, 
  level 3/.style = {sibling distance=6cm},
  level 4/.style = {sibling distance=3cm}
]
\node [arn_small] {10}
  child { node [arn_small] {2} 
    child {node [rectangle,draw] {$x<2$}    
    }
    child {node [arn_small] {6}
      child {node [arn_small] {4}
        child {node [rectangle,draw]{$2<x<4$}}
        child {node [rectangle,draw]{$4<x<6$}}    
      }
      child {node [arn_small]{8}
        child {node [rectangle,draw]{$6<x<8$}}
        child {node [rectangle,draw]{$8<x<10$}}    
      }
    }   
  }
  child {node [arn_small] {12}    
    child {node [rectangle,draw] {$10<x<12$}
    }
    child {node [rectangle,draw] {$x>12$}
    }  
  };
\end{tikzpicture}
\end{center}
\end{tiny}

\end{frame}

\begin{frame} \frametitle{ApproxMWPaging Phase 4.}

\begin{tiny}
\begin{center}
    \begin{tabular}{ | l | l | l | l |}
    \hline
    Memory Location & Node & Left Child Location & Right Child Location \\ \hline
    1  & 10  & 2    & 3    \\ \hline
    2  & 2  & 4    & 5    \\ \hline
    3  & 12 & 6    & 7    \\ \hline
    4  & $x < 2$  & ---   & ---    \\ \hline
    5  & 6  & 8   & 9   \\ \hline
    6  & $10 < x < 12$ & ---   & ---   \\ \hline
    7  & $x > 12$ & ---   & ---   \\ \hline
    8  & 4  & 10 & 11 \\ \hline
    9  & 8  & 12 & 13 \\ \hline
    10 & $2 < x < 4$  & --- & --- \\ \hline
    11 & $4 < x < 6$  & --- & --- \\ \hline
    12 & $6 < x < 8$  & --- & --- \\ \hline
    13 & $8 < x < 10$ & --- & 16   \\ \hline
    \end{tabular}
\end{center}

\end{tiny}
\end{frame}


\begin{frame} \frametitle{Expected Cost ApproxMWPaging} \label{45}
\begin{itemize}
\item[1.] We bind the depth of each key/gap based on its probability in the MWT 
\item[2.] We bind the depth of each key/gap based on its probability in BST
\item[3.] We bind the location in the memory of each key/gap (since we pack via BFS)
\item[4.] We can then explicitly bound the expected cost of search of our tree.
\end{itemize}
\end{frame}

\begin{frame}\frametitle{MWT Depth Bound}
Let $T'$ be the MWT created in Phase 1. For page size $m_1$ we have \cite{bose2009efficient}:
\begin{align*} d_{T'}(B_i) &\leq \lfloor log_{m_1}(\frac{1}{p_i}) \rfloor \\
d_{T'}((B_{i-1},B_i)) &\leq \lfloor log_{m_1}(\frac{2}{q_i}) \rfloor + 1 \text{ for all gaps, and}\\
d_{T'}((B_{i-1},B_i)) &\leq \lfloor log_{m_1}(\frac{1}{q_i}) \rfloor + 1 \\ 
&\text{ for at least $m$ of them (and the two extremal gaps).}
 \end{align*}
\end{frame}

\begin{frame}\frametitle{BST Depth Bound (Lemma 4.5.1)}
Let $T$ be the BST made in Phase 3.
\begin{align*} 
d_T(B_i) &\leq \lg(\frac{1}{p_i}) \\
d_T((B_{i-1},B_i)) &\leq \lg(\frac{1}{q_i}) + 2 \text{ for all gaps, and} \\
d_T((B_{i-1},B_i)) &\leq \lg(\frac{1}{q_i}) + 1 \\
&\text{for at least  $m$ of them (and the two extremal gaps).}
\end{align*}
\begin{proof}
Depth within each page at most $\lfloor \lg(m_1) \rfloor$. Multiplying by the "page depth" gives the desired result.
\end{proof}

\end{frame}

\begin{frame}\frametitle{Memory Location Bound (Lemma 4.5.2)}
For any key $B_i$, if
\begin{align*}
k=&min_{j \in \{1, ..., h\}} \mid m'_j \geq location(B_i) \text{ then} \\
&k=min_{j \in \{1, ..., h\}} \mid m'_j \geq \frac{2}{p_i}-1.
\end{align*}
(I've omitted the result for gaps)
\begin{proof}
Give the depth of a node in $T$, we calculate its maximum BFS rank, which gives us its maximum memory level.
\end{proof}
\end{frame}

\begin{frame}\frametitle{Explicit Search Cost (Lemma 4.5.3)}
The cost of searching for key $B_i$, $C(B_i)$,can be bounded as follows: 
\begin{align*} 
C(B_i) \leq  \sum_{k'=1}^{k-1} &\left(\lfloor \lg(m'_{k'}+1) \rfloor - \lfloor \lg(m'_{k'-1}+1) \rfloor \right)\cdot c_{k'} + \\
 &\left(\lg(\frac{1}{p_i}) + 1 - \lfloor \lg(m'_{k-1}+1) \rfloor \right)\cdot c_k \\
&\text{such that } \left( k=min_{j \in \{1, ..., h\}} \mid m'_j \geq \frac{2}{p_i}-1 \right) 
\end{align*}
This, along with the bound for gaps is summed over all nodes to give a complete equation (Theorem 4.5.4).
\end{frame}

\begin{frame}\frametitle{Cost in terms of Entropy (Theorem 4.5.8)}
\begin{align*}
C &< \left(H + 1 + \sum_{i=0}^n q_i - q_0 - q_n - \sum_{i=0}^m q_{\text{rank}[i]} \right) \cdot  c_h
\end{align*}
where $q_{\text{rank}[i]}$ is the $i^{th}$ smallest access probability among gaps except $q_0$ and $q_n$.
\begin{proof}
\begin{itemize}
\item Can bound cost of search under standard model.
\item Increasing costs as you move down in the tree.
\item Costs strictly less than $i \cdot c_h$ to access a node at depth $i$
\end{itemize}
\end{proof}
\end{frame}


\begin{frame} \frametitle{Algorithm ApproxBSTPaging}
Our second solution to create an approximately optimal BST under the HMM model works as follows: \\

\begin{enumerate}
\item Create a BST $T$ with an algorithm of De Prisco and De Santis \cite{de1993binary}: \textbf{$\mathbf{O(n)}$}. \\

\item Pack keys from $T$ into memory in a BFS order: \textbf{$\mathbf{O(n)}$}.
\end{enumerate}

Total time: $\mathbf{O(n)}$
\end{frame}

\begin{frame} \frametitle{Approximate Binary Search Trees of De Prisco and De Santis with Extensions by Bose and Dou\"{i}eb} \label{sec:deBST}

\begin{itemize}
\item[\textbf{Phase 1}] Create probability distribution with $2n$ zero, and $2n+1$ original probabilities. Use Bose and Dou\"{i}eb's BST algorithm to create an alphabetic tree.

\item[\textbf{Phase 2}] Move $p_i$ keys up \textit{starting tree} to the LCA of keys $q_{i-1}$ and $q_i$.

\item[\textbf{Phase 3}] Remove redundant edges.
\end{itemize}
\end{frame}


\begin{frame} \frametitle{Expected Cost ApproxBSTPaging}\label{48}
\begin{itemize}
\item[1.] We bind the depth of each key/gap based on its probability in BST
\item[3.] We bind the location in the memory of each key/gap (since we pack via BFS)
\item[4.] We can then explicitly bound the expected cost of search of our tree.
\end{itemize}
\end{frame}

\begin{frame}\frametitle{BST Depth Bound (Lemma 4.6.1)}
We define $R$:
\begin{align*}
R = \{m' \text{ smallest access probabilities}\} \cup \{q_0, q_n\}
\end{align*}
\begin{align*} 
d_T(B_i) \leq
\begin{cases}  
\lfloor \lg(\frac{1}{p_i}) \rfloor - 1 &\text{ if } p_i \in R  \\
\lfloor \lg(\frac{1}{p_i}) \rfloor &\text{ otherwise.}
\end{cases} \\
d_T((B_{i-1},B_i)) \leq 
\begin{cases}  
\lfloor \lg(\frac{1}{q_i}) \rfloor &\text{ if } q_i \in R  \\
\lfloor \lg(\frac{1}{q_i}) \rfloor + 2 &\text{ if } q_i \text{ is } q_{max} \\
\lfloor \lg(\frac{1}{p_i}) \rfloor + 1 &\text{ otherwise.}
\end{cases}
\end{align*}


\begin{proof}
Result comes immediately from explanations within the work of Bose and Douou\"{i}eb \cite{bose2009efficient}.
\end{proof}

\end{frame}

\begin{frame}\frametitle{Memory Location Bound (Lemma 4.8.1)}
For any key $B_i$, if
\begin{align*}
k=&min_{j \in \{1, ..., h\}} \mid m'_j \geq location(B_i) \text{ then} \\
&k=min_{j \in \{1, ..., h\}} \mid m'_j \geq \frac{1}{p_i}-1 \text{ if } p_i \in R \\
&k=min_{j \in \{1, ..., h\}} \mid m'_j \geq \frac{2}{p_i}-1 \text{ otherwise.}
\end{align*}
(I've omitted the result for gaps)
\begin{proof}
Give the depth of a node in $T$, we calculate its maximum BFS rank, which gives us its maximum memory level.
\end{proof}
\end{frame}

\begin{frame}\frametitle{Explicit Search Cost (Lemma 4.8.2)}
\begin{align*} 
C(B_i) \leq 
\begin{cases}
 \sum_{k'=1}^{k-1} &\left(\lfloor \lg(m'_{k'}+1) \rfloor - \lfloor \lg(m'_{k'-1}+1) \rfloor \right)\cdot c_{k'}+ \\
 &\left(\lg(\frac{1}{p_i}) - \lfloor \lg(m'_{k-1}+1) \rfloor \right)\cdot c_k\\
&\text{such that } k=min_{j \in \{1, ..., h\}} \mid m'_j \geq \frac{1}{p_i}-1 \text{ if } p_i \in R \\\\
 \sum_{k'=1}^{k-1} &\left(\lfloor \lg(m'_{k'}+1) \rfloor - \lfloor \lg(m'_{k'-1}+1) \rfloor \right)\cdot c_{k'}+ \\
 &\left(\lg(\frac{1}{p_i}) + 1 - \lfloor \lg(m'_{k-1}+1) \rfloor \right)\cdot c_k\\
&\text{such that } k=min_{j \in \{1, ..., h\}} \mid m'_j \geq \frac{2}{p_i}-1 \text{ otherwise.}  
\end{cases}
\end{align*}
This, along with the bound for gaps is summed over all nodes to give a complete equation (Theorem 4.8.3).
\end{frame}

\begin{frame}\frametitle{Cost in terms of Entropy (Theorem 4.8.5)}
\begin{align*}
C &<  (H + 1 - q_0 - q_n + q_{max} - \sum_{i=0}^{m'} pq_{\text{rank}[i]})\cdot c_h
\end{align*}
\begin{proof}
\begin{itemize}
\item Can bound cost of search under standard model.
\item Increasing costs as you move down in the tree.
\item Costs strictly less than $i \cdot c_h$ to access a node at depth $i$
\end{itemize}
\end{proof}
\end{frame}


%=========================================
\section{Binary Trees on Unordered Sequences of Probabilities}\label{BST over Multisets}

\begin{frame} \frametitle{Binary Trees on Unordered Sequences of Probabilities}
We have a multiset of $n$ probabilities (n is odd): \\
$M = \lbag p_1, p_2, ..., p_n \rbag$ such that $\sum\limits_{i=1}^n p_i = 1$.  \\
The cost under \textit{\textbf{S}tandard Model}:
\begin{equation}
C_{T_S} = \sum_{i=1}^{n} p_i(b_i+1) - \sum_{p_i \in L_T} p_i
\end{equation}
$b_i$ is the depth of the node representing probability $p_i$.
\end{frame}

\begin{frame} \frametitle{Binary Trees on Unordered Sequences of Probabilities}
\begin{itemize}
\item Similar to Huffman code problem but we can place probabilities and internal \textit{and} external node locations.
\item As in previous models, we don't charge to "examine" the leaves.
\item The above constraint makes finding the optimal solution much more difficult.
\item We show our GREEDY-MS Algorithm is Within $\frac{n+1}{2n}$ of Optimal.
\end{itemize}
\end{frame}



\begin{frame} \frametitle{The \textbf{E}xpected Path Length Model}
Consider a new cost model for this problem, the \textit{\textbf{E}xpected Path Length Model} (EPL), which has cost $C_{T_E}$ defined as follows:
\begin{equation}
C_{T_E} = \sum_{i=1}^{n} p_i(b_i+1)
\end{equation}
This model treats internal and external nodes the same.
\end{frame}

\begin{frame} \frametitle{GREEDY-MS}
There is a simple, greedy, $O(n \lg n)$ that is optimal under EPL \cite{golin2012huffman}. We call this algorithm \textit{GREEDY-MS}:

\begin{itemize}
\item[1.] Sort the multiset $M$ to create $R$.

\item[2.] The root of our tree will be $R[0]$, its two children will be $R[1]$ and $R[2]$, and so on. $R[i]$ will be placed at location $i+1$ in BFS order.
\end{itemize}
\end{frame}

\begin{frame}\frametitle{GREEDY-MS optimal under EPL (Lemma 5.2.1)}
\textbf{Claim 1.}\label{Claim-EPL}
For a given depth $d$, the probabilities of all nodes on level $d+1$ are greater than or equal to the probabilities of all nodes on level $d$.
\begin{proof}
Otherwise we could swap contradicting probabilities and get a better tree.
\end{proof}

\noindent \textbf{Claim 2.}\label{Claim-EPL2}
$T'$ must be full at all depths strictly less than its height.
\begin{proof}
Otherwise we could place a leaf at an non-full level and get a better tree.
\end{proof}

Taking claims 1 and 2 together gives the result.
\end{frame}

\begin{frame}{GREEDY-MS within $\frac{n+1}{2n}$ of optimal under \textit{Standard Model} }
\begin{itemize}
\item[1.] We show that in any optimal tree under the EPL model all but the smallest leaf can be \textit{covered}.
\item[2.] We show that for an optimal tree under the EPL model its leaves have probability at most $\frac{n+1}{2n}$.
\item[3.] We show that if a tree existed which was more than  $\frac{n+1}{2n}$ better than GREEDY-MS under the \textit{Standard Model}, we would get a contradiction.
\end{itemize}
\end{frame}

\begin{frame}{Covering Lemma (Lemma 5.2.2)}
An internal node with probability $p_i$ \textit{covers} a node $p_j$ if $p_i > p_j$.\\
For a tree $T$ with all parents greater than children:
\begin{align*}
\forall_{p_i \in L_T-\{l_{min}\}} \exists \text{ unique } p_i' \notin \{L_T \cup \{l_{min}\}\} \text{ such that } p_i' \text{ covers } p_i
\end{align*}
PROOF BY INSERT PICTURE

\end{frame}

\begin{frame}{Bound of Leaf probabilities (Lemma 5.2.3)}
For a tree $T$ with all parents greater than children:
\begin{align*}
\sum_{p_i \in L_T} p_i \leq \frac{n+1}{2n}
\end{align*}
\begin{proof}
Since each leaf except the smallest is covered:
\begin{align*}
\sum_{p_i \in L_T} p_i \leq \frac{1}{2} \cdot (1-l_{min}) + l_{min}
\end{align*}
$l_{min}$ is at most $\frac{1}{n}$ so we get the desired result.
\end{proof}
\end{frame}

\begin{frame}{GREEDY-MS within $\frac{n+1}{2n}$ of optimal under \textit{Standard Model}}
Let $T$ be our tree made by GREEDY-MS. If there existed a tree $T'$ which had cost:
\begin{align*}
C_{T'_S} < C_{T_S} - \frac{n+1}{2n}
\end{align*}
Using the previous Lemma, we would contradict the optimality of $T$ under the EPL Model.
\end{frame}


%=========================================



\section{Conclusion and Open Problems} \label{Conclusion and Open Problems}

\begin{frame} \frametitle{Conclusion}
\begin{itemize}
\item Modified Entropy Rule first proposed by  G{\"u}ttler, Mehlhorn and Schneider in 1980 had a worst case expected cost of $H+4$ (previous $c\cdot H+2$ where $c \approx 1.08$).

\item ApproxMWPaging and ApproxBSTPaging solve the problem "well" under the HMM in $O(n\cdot \lg(m_1))$ and $O(n)$ respectively.

\item GREEDY-MS is within $\frac{n+1}{2n}$ when given unordered probabilities under the \textit{Standard Model}.   
\end{itemize}

\end{frame}


\begin{frame}{Open Problems}
\begin{itemize}
\item Is the modified entropy rule at most $H+2$?
\item Can ApproxMWPaging and ApproxBST be extended to more sophisticated models (copying/movement within the hierarchy)?
\item Is there a polynomial time algorithm which solves the unordered problem optimally under the \textit{Standard Model}?
\item Are there better "fast" heuristics for this problem?
\end{itemize}
\end{frame}


\bibliographystyle{abbrv}
\bibliography{uw-ethesis/uw-ethesis}


\end{document}
