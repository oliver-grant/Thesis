\documentclass[]{beamer}

\usepackage{beamerthemesplit} 
\usepackage{tikz}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{pifont}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{enumerate}
\usepackage{amsfonts}
\usepackage{url}
\usepackage{amsmath,amsfonts,amssymb,amsthm,epsfig,epstopdf,url,array}
\usepackage[]{units}
\usepackage{xcolor}
\usepackage{fancybox}
\usepackage{tikz}
\usepackage{stmaryrd}
\usepackage{caption}
\usepackage{subcaption}
\theoremstyle{plain}
\newtheorem{thm}{Theorem}[section]
\newtheorem{lem}[thm]{Lemma}
\newtheorem{prop}[thm]{Proposition}
\newtheorem*{cor}{Corollary}

\newcommand*\rfrac[2]{{#1}/{#2}}
\makeatletter
\newcommand{\rom}[1]{\romannumeral #1}

\definecolor{RazerGreen}{RGB}{71,225,12}
\definecolor{RazerGreen}{RGB}{71,225,12}
\setbeamercolor{title}{fg=black}
\setbeamercolor{frametitle}{fg=black}
\setbeamercolor{structure}{fg=RazerGreen}

\tikzset{
    current/.style={
		scale=0.8,
		fill=blue,
		minimum size=0.5pt,
        draw,
        circle,
    }
}
\tikzset{
    select/.style={
		scale=0.8,
		fill=red,
		minimum size=0.5pt,
        draw,
        circle,
    }
}
\tikzset{
    max/.style={
		scale=0.8,
		minimum size=0.5pt,
        draw,
        circle,
    }
}

\tikzset{
  treenode/.style = {align=center, inner sep=0pt, text centered,
    font=\sffamily\small},
  arn_n/.style = {treenode, circle, draw=black,
    fill=white, text width=2em},% arbre rouge noir, noeud noir
  arn_small/.style = {treenode, circle, draw=black,
    fill=white, text width=2em, font=\tiny, align=center},
  arn_small_r/.style = {treenode, rectangle, draw,
    fill=white, font=\tiny, align=center},
  arn_r/.style = {treenode, circle, red, draw=red, 
    text width=2em, very thick},% arbre rouge noir, noeud rouge
  arn_x/.style = {treenode, rectangle, draw=black,
    minimum width=2.2em, minimum height=2.5em}% arbre rouge noir, nil
}

\title{Approximately Optimum Search
Trees in External Memory Models}
\subtitle{Master of Mathematics Presentation}
\author{Oliver Grant \\
Supervisor: Ian Munro}



\begin{document} \date{}
\begin{frame}
  \titlepage
\end{frame}

%\tableofcontents

\section{Introduction}

\begin{frame} \frametitle{Binary Search Trees}

\begin{itemize}

\item \textbf{BST} - Simple structure used to store key-value pairs

\item First appeared in the late 1950s/early 1960s

\item Attributed to Windley, Booth, Colin and Hibbard \cite{windley1960trees, booth1960efficiency, hibbard1962some}

\item Ordering property over keys allows for quick searches

\end{itemize}
\end{frame}

\begin{frame} \frametitle{The Optimum Binary Search Tree Problem}

\begin{itemize}

\item Knuth proposed optimum BST problem in 1971 \cite{knuth1971optimum}

\item $n$ keys, $B_1, B_2, ..., B_n$

\item $2n+1$ frequencies, ${p_1, p_2, ..., p_n}$, ${q_0, q_1, ..., q_n}$

\item $p_i$, probability of searching for $B_1$

\item $q_i$, probability of searching for key in gap $(B_i, B_{i+1})$

\item We assume $\sum\limits_{i=1}^n p_i + \sum\limits_{i=0}^n q_i = 1$

\item Also assume w.l.o.g. $q_{i-1}+p_i+q_i \neq 0$

\item Keys must be internal nodes, gaps leaves

\item $P = \sum_{i=1}^{n} p_i \cdot (d_T(B_i)+1) + \sum_{i=0}^{n} q_i \cdot(d_T((B_i, B_{i+1})))$


\end{itemize}
\end{frame}


\begin{frame} \frametitle{Optimum BST Example}
\begin{center}
\begin{tikzpicture}[level/.style={sibling distance = 6cm/####1,
  level distance = 1.5cm}] 
\node [arn_n] {0.15 THE}
    child{ node [arn_n] {0.10 BE} 
            child{ node [arn_x] {0.10 \\ $x < BE$}} 
            	child{ node [arn_x] {0.40 \\  $BE < x < THE$}}
            }
    child{ node [arn_n] {0.10 TO}
            child{ node [arn_x] {0.01 \\ $THE < x < TO$}}
            child{ node [arn_x] {0.14 \\ $x > TO$}}
		}
		
; 
\end{tikzpicture}



{\scriptsize
\begin{align*}
P = &\sum_{i=1}^{n} p_i \cdot (d_T(B_i)+1) + \sum_{i=0}^{n} q_i \cdot(d_T((B_i, B_{i+1}))) \\
= &0.15 \cdot 1 + 0.10 \cdot 2 + 0.10 \cdot 2 + 0.10 \cdot 2 + 0.10 \cdot 2 + 0.40 \cdot 2 + 0.01 \cdot 2 + 0.14 \cdot 2 \\
= &1.85
\end{align*}
}%

\end{center}
\end{frame}

\begin{frame} \frametitle{Three-Way Branching}

\begin{center}
\begin{tikzpicture}
\node[arn_n] {0.4 a}
  child {node [arn_n] {0.3 b}    
  }  
  child {node [arn_n] {0.3 c}
  };
\end{tikzpicture}
\end{center}

\begin{itemize}



\item \definecolor{lightgrey}{rgb}{0.95,0.92,0.92} % Defines the color used for content box headers
\colorbox{lightgrey}{ \fontfamily{cmtt}\selectfont \uppercase{IF (expr) label1, label2, label3} } 

\item Control transferred to one of $3$ locations with one statement

\item Always single comparison under three-way branching

\item Minimum $1.6$ comparisons expected using $<, >, =$
\end{itemize}
\end{frame}

\iffalse

\begin{frame} \frametitle{Why Study Binary Search Trees}


\begin{itemize}
\item AVL trees self-balancing BST in 1963, maintains $O(\lg(n))$ height \cite{adelsonvelskii1963algorithm}

\item B-trees/Red-black trees by R. Bayer in 1972 (with later extensions by others) \cite{bayer1972symmetric}

\item  Allen and Munro examined a MTR heuristic \cite{allen1978self}

\item Splay trees of Sleator and Tarjan in 1985, dynamic optimality conjecture \cite{sleator1985self}

\item Tango trees in 2007 by Demaine et al., $O(\lg \lg n)$-competitive \cite{demaine2007dynamic}

\end{itemize}

\end{frame}
\fi

\begin{frame} \frametitle{Why Study Binary Search Trees}

\begin{itemize}

\item Binary space partitions in 3D graphics \cite{schumacker1969study, paterson1992optimal}

\item Binary tries in routers and IP lookup structures \cite{song2010building}

\item Programming languages like C++ \texttt{std::map} \cite{CppMap} 

\item Syntax trees during compilation \cite{louden1997compiler}

\item Many more examples

\item Theoretically interesting

\end{itemize}

\end{frame}


\begin{frame}{Overview}

\begin{itemize}

\item Review work on BSTs, multiway trees, and alphabetic trees

\item We have three separate results:
\end{itemize}

\begin{enumerate}
\item In the standard problem, we improve bound on MME Heuristic of G{\"u}ttler, Mehlhorn and Schneider \cite{guttler1980binary}

\item Under Hierarchical Memory Model \cite{aggarwal1987model}, two $O(n)$ time approximation algorithms

\item Given unordered probabilities show $O(n)$ algorithm within $\frac{n+1}{2n}$ of optimal
\end{enumerate}




\end{frame}

\section{Background and Related Work} \label{Background and Related Work}

\begin{frame} \frametitle{Optimum Binary Search Trees}

\begin{itemize}
\item In 1971 C. Gotlieb and Walker approximate solution \cite{walker1971top}

\item Knuth $O(n^2)$ time and space solution \cite{knuth1971optimum}

\item Approximate solutions followed

\item Interestingly, it is hard to bound the cost of approximation algorithms to the optimal

\item Discuss in terms of \textit{entropy}:
\end{itemize}

\begin{align*}
H = \sum_{i=1}^{n} p_i\cdot\lg(\frac{1}{p_i}) + \sum_{j=0}^{n} q_i\cdot\lg(\frac{1}{q_j})
\end{align*}

\end{frame}

\begin{frame} \frametitle{Approximately Optimum Binary Search Trees}


\begin{itemize}
\item In 1975, P. Bayer showed \cite{bayer1975improved}:
\begin{align*}
H-\lg H-(\lg e-1) \leq C_{Opt} \leq C_{WB}, C_{MM} \leq H + 2
\end{align*}

\item $C_{Opt}$ optimal solution

\item $C_{WB}, C_{MM}$ both top-down greedy approaches running in $O(n)$

\item $P_L(B_i)$,  $P_R(B_i)$ probabilities of searching for key before or after $B_i$ respectively

\item $C_{WB}$ weight-balanced heuristic: \\
Select root $B_i$ as root with minimum $|P_L(B_i)-P_R(B_i)|$

\item $C_{MM}$ min-max heuristic:  \\
Select $B_i$ as root with minimum $\max(P_L(B_i), P_R(B_i))$


\end{itemize}

\end{frame}

\begin{frame} \frametitle{Approximately Optimum Binary Search Trees}

\begin{itemize}

\item In addition to using $H$ to represent the entropy of our whole distribution:
\begin{center}
$H(x_1,x_2,...,x_n) = \sum_{i=1}^{n} x_i\cdot\lg(\frac{1}{x_i})$
\end{center}

\item 1980 G{\"u}ttler, Mehlhorn and Schneider gave new heuristic: \\
\textbf{The Modified Minimum Entropy Heuristic}

\item Greedy top-down approach (basic idea):

\item Selects root $B_i$ such that $H(P_L(B_i), p_i, P_R(B_i))$ is maximized

\item $O(n^2)$ time, $O(n)$ space

\item Unfortunately, best upper bound: $C_{ME} \leq c_1\cdot H+2$ \\ where $c_1=\frac{1}{H(\frac{1}{3}, \frac{2}{3})} \approx 1.08$ 

\item We show the expected cost is $C_{ME} \leq H+4$


\end{itemize}

\end{frame}

\begin{frame} \frametitle{Approximately Optimum Binary Search Trees}

\begin{itemize}
\item In 1993 De Prisco and De Santis gave a new heuristic \cite{de1993binary}

\item Worst case expected cost: $H+1-q_0-q_n+q_{max}$ \\
 where $q_{max}$ is the maximum weight leaf node

\item Updated in 2009 by Bose and Dou\"{i}eb to: \\
$H + 1 - q_0 - q_n + q_{max} - \sum_{i=0}^{m'} pq_{\text{rank}[i]}$

\item $m'=\max({2n-3P,P})-1 \geq \frac{n}{2} - 1$ 

\item $P$ is number of increasing or decreasing sequences in a left-to-right read of the access probabilities of the leaves

\item $pq_{\text{rank}[i]}$ is the $i^{th}$ smallest access probability among all keys and leaves except $q_0$ and $q_n$.


\end{itemize}

\end{frame}

 
\begin{frame} \frametitle{Alphabetic Trees}
\begin{scriptsize}
\begin{center}
\begin{tikzpicture}[scale = 0.6]
\node[arn_small] {}
  child {node [arn_small] {}
    child {node [arn_small] {0.3 BE}}
    child {node [arn_small] {0.4 THE}}
  }  
  child {node [arn_small] {0.3 TO}
  };
\end{tikzpicture}
\end{center}
\end{scriptsize}

\begin{itemize}
\item Given $n$ keys with $\sum_{i=1}^{n} p_i = 1$

\item $n$ keys are leaves, every internal node has two children

\item We wish to minimize expected search cost: $\sum_{i=1}^{i=n} p_i \cdot d_T(B_i)$


\item Similar to Huffman code, but have an ordering over keys \cite{huffman1952method}

\end{itemize}

\end{frame}

\begin{frame}\frametitle{Alphabetic Trees}
\begin{itemize}

\item Independent optimal solutions due to Hu and Tucker \cite{hu1971optimal} and Garsia and Wachs \cite{garsia1977new}: $O(n \lg n)$ time and $O(n)$ space

\item Both went through proof simplifications \cite{knuth1973sorting, hu1973new, hu1979binary} and \cite{kingston1988new}

\item Eventually shown to be equivalent in 1982 by Hu \cite{Hu1982Book} 

\item 1991 Yeung gave $O(n)$ approximation with worst-case expected cost at most: $H + 2 - p_1-p_n$ \cite{yeung1991alphabetic}

\item In 1993 improved by De Prisco and De Santis to \cite{de1993binary}: $H+1-p_1-p_n+p_{max}$

\item Final improvement in 2009 by Bose and Dou\"{i}eb \cite{bose2009efficient}:
$H+1 -p_1-p_n-\sum_{i=0}^m p_{\text{rank}[i]} + p_{max}$


\end{itemize}
\end{frame}



\begin{frame} \frametitle{Multiway/K-ary trees} \label{sec:MWT}
\begin{tiny}
\begin{center}

\begin{tikzpicture}[scale=0.9]

\node [circle,draw] (z){2\: 10 12}
  child {node [rectangle,draw] (a) {$x<2$}    
  }  
  child {node [circle,draw] (b) {4\: \:6\: \:8}
    child {node [rectangle,draw] (a) {$2<x<4$}}
    child {node [rectangle,draw] (a) {$4<x<6$}}
    child {node [rectangle,draw] (a) {$6<x<8$}}
    child {node [rectangle,draw] (a) {$8<x<10$}}
  }  
  child {node [rectangle,draw] (c) {$10<x<12$}
  }
  child {node [rectangle,draw] (d) {$x>12$}
  };

\end{tikzpicture}

\end{center}
\end{tiny}

\begin{itemize}

\item Given $n$ keys and $n+1$ gaps with probabilities

\item Up to $k-1$ keys in internal node, single gap in a leaf node

\item Cost of search within an internal node is constant

\item Expected cost of search is still: \\
$\sum_{i=1}^{n} p_i \cdot (d_T(B_i)+1) + \sum_{j=0}^{n} q_j \cdot d_T((B_{i-1},B_i))$


\end{itemize}

\end{frame}

\begin{frame}

\begin{itemize}

\item Vishnavi et al. \cite{vaishnavi1980optimum}, and Gotlieb  \cite{gotlieb1981optimal} independently solved optimally in $O(k\cdot n^3)$ time

\item In 1997, Becker propsed an $O(Dkn)$ time solution where D is the height of the tree \cite{becker1997construction}

\item Thought, but not proven, to be close to optimal

\item Bose and Dou\"{i}eb's 2009 work also solved this problem with an $O(n)$ solution with worst case expected cost:
\begin{scriptsize}
\begin{align*}
\frac{H}{\lg(2k-1)} \leq P_{OPT} \leq P_T \leq \frac{H}{\lg k} + 1 + \sum_{i=0}^n q_i - q_0 - q_n - \sum_{i=0}^m q_{\text{rank}[i]}
\end{align*}
\end{scriptsize}

\end{itemize}


\end{frame}


\section{An Improved Bound for the Modified Minimum Entropy Heuristic}\label{An Improved Bound for the Modified Minimum Entropy Heuristic}

\begin{frame} \frametitle{An Improved Bound for the Modified Minimum Entropy Heuristic}

\begin{itemize}

\item Modified Minimum Entropy (ME) Heuristic of G{\"u}ttler, Mehlhorn and Schneider \cite{guttler1980binary}

\item Previous bound: $C_{ME} \leq c_1\cdot H+2$ \\ where $c_1=\frac{1}{H(\frac{1}{3}, \frac{2}{3})} \approx 1.08$ 

\item We show the expected cost is $C_{ME} \leq H+4$

\end{itemize}

\end{frame}


\begin{frame} \frametitle{Preliminaries}

\begin{itemize}

\item For subtree $t$, we let 
\begin{equation}
p_t=\sum_{i : B_i \in t} p_i + \sum_{i : (B_i, B_{i+1}) \in t} q_i
\end{equation}
\item $P_{L_t}(B_i)$, $P_{R_t}(B_i)$: normalized probabilities of searching before or after $B_i$:
\begin{equation}
P_{L_t}(B_i) = \frac{\sum_{i : B_i \in t} p_i + \sum_{i : (B_i, B_{i+1}) \in t} q_i}{p_t}.
\end{equation}
\item $P_{L_t}(B_i,B_{i+1})$ and $P_{R_t}(B_i,B_{i+1})$ analogous. \\
\item The local entropy of subtree $t$ is:
\begin{equation}
E_t=H(P_{L_t}(B_i), \frac{p_i}{p_t}, P_{R_t}(B_i))
\end{equation} 

\end{itemize}
\end{frame}


\begin{frame} \frametitle{The Modified Entropy Rule}\label{The Entropy Rule}

\begin{itemize}
\item The basic version of the hueristic greedily chooses root in a top-down manner:
\item \begin{center}
\textit{$B_i$ is selected such that $H(P_{L_t}(B_i), \frac{p_i}{p_t}, P_{R_t}(B_i))$ is maximized (as in the original entropy rule).}
\end{center}
\end{itemize}

\end{frame}

\begin{frame} \frametitle{The Entropy Rule's Shortcomings}

\begin{figure}[H]
\centering
% Set the overall layout of the tree
\tikzstyle{level 1}=[level distance=3cm, sibling distance=3cm]
\tikzstyle{level 2}=[level distance=3cm, sibling distance=3cm]
\scriptsize
\begin{subfigure}{.46\textwidth}
\centering
\begin{tikzpicture}[scale=0.5]
\node [circle,draw] (z){$\frac{1}{5}$}
  child {node [circle,draw] (a) {$\frac{1}{5}$}    
  }  
  child {node [circle,draw] (b) {$0$}
	  child {node [circle,draw] (e) {$0$}}
	  child {node [circle,draw] (f) {$\frac{3}{5}$}}
  };
\end{tikzpicture}
\caption{Entropy rule tree: $C=\frac{8}{5}$}
\end{subfigure}
\begin{subfigure}{.46\textwidth}
\centering
\begin{tikzpicture}[scale=0.5]
\node [circle,draw] (z){$0$}
  child {node [circle,draw] (a) {$\frac{1}{5}$} 
  	child {node [circle,draw] (e) {$\frac{1}{5}$}}
	  child {node [circle,draw] (f) {$0$}}   
  }  
  child {node [circle,draw] (b) {$\frac{3}{5}$}
  };
\end{tikzpicture}
\caption{Modified entropy rule tree: $C=\frac{7}{5}$}
\end{subfigure}
\caption{Comparison of entropy and modified entropy rule heuristics}
\end{figure}

Given the probability set $\{q_0 = \frac{1}{5}, p_1 = \frac{1}{5}, q_1 = 0, p_2 = 0, q_3 = \frac{3}{5}\}$ the entropy rule will mistakenly choose key $B_1$ as the root while selecting $B_2$ as the root produces a better tree.

\end{frame}

\begin{frame} \frametitle{The Modified Minimum Entropy Rule}

\begin{itemize}
\item[\textit{a)}] If there exists key $B_i$ such that $\frac{p_i}{p_t} > \max(P_{L_t}(B_i), P_{R_t}(B_i))$ we always select $B_i$ as the root.

\item[\textit{b)}] If there exists a gap $(B_i, B_{i+1})$ such that $\frac{q_i}{p_t} > max(P_{L_t}(B_i, B_{i+1}), P_{R_t}(B_i, B_{i+1}))$ then we select the root from among $B_i$ and $B_{i+1}$. $B_i$ is chosen if $P_{L_t}(B_i, B_{i+1}) > P_{R_t}(B_i, B_{i+1})$ and $B_{i+1}$ is chosen otherwise.

\item[\textit{c)}] Otherwise, $B_i$ is selected such that $H(P_{L_t}(B_i), \frac{p_i}{p_t}, P_{R_t}(B_i))$ is maximized (as in the original entropy rule).

\end{itemize}
The approach proposed by G{\"u}ttler, Mehlhorn and Schneider takes $O(n^2)$ time in the worst case and $O(n)$ space.
\end{frame}


\begin{frame} \frametitle{Modified Entropy is Within 4 of Entropy}

\begin{itemize}
\item We start with two important equations:
\end{itemize}
\begin{align*}
C_{ME} &= \sum_{t \in S_T} p_t
 &H = \sum_{t \in S_T} p_t \cdot E_t
\end{align*}  
where $S_T$ is the set of all subtrees of our tree $T$. \\

\noindent For each subtree, we bind $E_t \geq 1 - \frac{b_t}{p_t}$ (we define $b_t$ later): 
\begin{align*}
&H = \sum_{t \in S_T} p_t E_t \geq \sum_{t \in S_T} p_t \cdot (1 - \frac{b_t}{p_t}) = C - \sum_{t \in S_T} b_t \\
 \implies &C \leq H + \sum_{t \in S_T} b_t
\end{align*}

\end{frame}

\begin{frame}

\begin{itemize}
\item For each root, we show that one of three cases must occur:

\item[\textit{Case 1)}] $E_t \geq 1-2 \frac{p_r}{p_t}$. \textbf{We set $b_t = 2p_r$.}

\item[\textit{Case 2)}] There exists gap $(B_i, B_{i+1})$ such that $\frac{q_i}{p_t} > max(P_{L_t}(B_i, B_{i+1}), P_{R_t}(B_i, B_{i+1}))$. \textbf{We set $b_t = q_i$.}

\item[\textit{Case 3)}]  $\max(P_{L_t}(B_r), P_{R_t}(B_r)) < \frac{4}{5}$. \textbf{We set $b_t = q_m$ where $q_m$ is the middle gap.}

\end{itemize}


\end{frame}

\begin{frame}

\textit{Case 1)} $E_t \geq 1-2 \frac{p_r}{p_t}$. \textbf{We set $b_t = 2p_r$.}


\begin{itemize}
\item "The easy case"
\item If we have a key in the middle this will follow.
\item Happens at most once per key.
\end{itemize}

\end{frame}
\begin{frame}
\textit{Case 2)} There exists gap $(B_i, B_{i+1})$ such that $\frac{q_i}{p_t} > max(P_{L_t}(B_i, B_{i+1}), P_{R_t}(B_i, B_{i+1}))$. \textbf{We set $b_t = q_i$.}
\begin{itemize}
\item Still fairly easy
\item If we have a big gap in the middle of our dataset, we use rule b).
\item Can happen at most twice for a gap.
\end{itemize}
\end{frame}

\begin{frame}
\textit{Case 3)} $\max(P_{L_t}(B_r), P_{R_t}(B_r)) < \frac{4}{5}$. \textbf{We set $b_t = q_m$ where $q_m$ is the middle gap.}

\begin{itemize}
\item The hard case
\item When this happens, since $\max(P_{L_t}(B_r), P_{R_t}(B_r)) < \frac{4}{5}$, our middle gap must get bigger relative to the remaining probability.
\item once it is big enough ($\geq \frac{1}{2}$), case 2 occurs.
\item We can show that $E_t \geq 1 - \frac{4}{5}\frac{q_m}{p_t}^2$ so we set $b_t = \frac{4}{5}\frac{q_m^2}{p_t}$ 
\item Each gap can contribute at most:
\begin{align*}
q_m \cdot \sum\limits_{x=0}^{\infty} \frac{1}{2} \cdot (\frac{4}{5}) ^ x = \mathbf{q_m \cdot 2}
\end{align*}
\end{itemize}

\end{frame}

\begin{frame}

\begin{itemize}
\item Combining these three cases we get a total of:
\end{itemize}

\begin{align*}
C \leq H + \sum_{t \in S_T} b_t = H + 2 \sum\limits_{r = 1}^n p_r + 2 \sum\limits_{m = 0}^n q_m + \sum\limits_{m = 0}^n \sum\limits_{t \in S_m} \frac{4}{5}\frac{q_m^2}{p_t}
C &\leq H + 2 + \sum\limits_{m = 0}^n (\frac{4}{5} \cdot q_m) \sum\limits_{x=0}^{\infty} \frac{1}{2} \cdot (\frac{4}{5}) ^ x \\
C &\leq H + 2 + 2
C &\leq H + 4
\end{align*}
\end{frame}

\section{Approximate Binary Search in the Hierarchical Memory Model}\label{Approximate Binary Search in the Hierarchical Memory Model} 
 
\begin{frame} \frametitle{The Hierarchical Memory Model}\label{The Hierarchical Memory Model}

\begin{itemize}

\item HMM by Aggarwal et al. in 1987
\item While not perfect, improvement over the RAM model
\item Hierarchy of different memories, increasing in size and decreasing in access cost
\item INSERT PICTURE

\end{itemize}
\end{frame}

\begin{frame}
\begin{itemize}
\item Formally, $R_1, R_2, ...$ unlimited number of registers each with location in memory.
\item Set of memories $M_1, M_2, ...M_l$
\item With memory sizes $m_1, m_2, ..., m_l$
\item Cost of accessing an item in $M_i$ is $c_i$
\item We assume $c_1 < c_2 < ... < c_l$
\item Cost of accessing item at location $a$:
\begin{align*}
\mu (a) = c_i \text{ if } \sum_{j = 1}^{i-1}m_j  < a \leq \sum_{j = 1}^{i}m_j.
\end{align*}
\end{itemize}
\end{frame}

\begin{frame} \frametitle{Optimum Binary Search Trees on the HMM Model}
\begin{itemize}
\item Given $n$ keys and $n+1$ gaps with associated probabilities as before.  
\item Construct binary tree $T$ (as before).
\item Put assign the nodes in $T$ to memory locations
\item Minimize the expected cost of search:
\end{itemize}
\end{frame}

\begin{frame}
TODO INSERT COST OF ACCESS PICTURE WHICH IS SUPER NEEDED


\end{frame}

\begin{frame}[fragile] \frametitle{Algorithm ApproxMWPaging}\label{Algorithm ApproxMWPaging}

\begin{enumerate}
\item Create MWT using algorithm of Bose and Dou\"{i}eb with $m_1$ page size

\item Create balanced BST inside each node.

\item Connect the balanced BSTs to form a single BST

\item Pack into memory in BFS order.

\end{enumerate}

\noindent Left with a BST in our memory in $O(n\cdot\lg(m_1))$  \\
($O(n)$ if $m_1 \in O(1)$).

\end{frame}

\begin{frame} \frametitle{ApproxMWPaging Phase 1.}

\begin{tiny}
\begin{center}

\begin{tikzpicture}[scale=0.9]

\node [circle,draw] (z){2\: 10 12}
  child {node [rectangle,draw] (a) {$x<2$}    
  }  
  child {node [circle,draw] (b) {4\: \:6\: \:8}
    child {node [rectangle,draw] (a) {$2<x<4$}}
    child {node [rectangle,draw] (a) {$4<x<6$}}
    child {node [rectangle,draw] (a) {$6<x<8$}}
    child {node [rectangle,draw] (a) {$8<x<10$}}
  }  
  child {node [rectangle,draw] (c) {$10<x<12$}
  }
  child {node [rectangle,draw] (d) {$x>12$}
  };

\end{tikzpicture}

\end{center}
\end{tiny}

\end{frame}

\begin{frame} \frametitle{ApproxMWPaging Phase 2.}

% Set the overall layout of the tree
\tikzstyle{level 1}=[level distance=3cm, sibling distance=3.5cm]
\tikzstyle{level 2}=[level distance=3cm, sibling distance=3.5cm]

\tikzset{smnode/.style={level 1/.style={level distance=1cm, sibling distance=1cm}, circle, draw}}
\tikzstyle{level 1}=[level distance=3cm, sibling distance=3.5cm]
\tikzstyle{level 2}=[level distance=3cm, sibling distance=3.5cm]
\tikzstyle{level 3}=[level distance=1cm, sibling distance=1cm]

\begin{tiny}
\begin{center}

\begin{tikzpicture}[scale=0.5]

\node [circle,draw] (z){
  \tikzstyle{level 1}=[level distance=0.5cm, sibling distance=0.5cm]
	\begin{tikzpicture}
	\node [draw,circle] (zZ){10}
  	child {node [draw,circle] (Za) {2}    
  	}  
  	child {node [draw,circle] (Zb) {12}
  	};
	\end{tikzpicture}
}
  child {node [rectangle,draw] (a) {$x<2$}    
  }  
  child {node [circle,draw] (b) {
    \tikzstyle{level 2}=[level distance=0.5cm, sibling distance=0.5cm]
	\begin{tikzpicture}
	\node [draw,circle] (zZa){6}
  	child {node [draw,circle] (Zaa) {4}    
  	}  
  	child {node [draw,circle] (Zba) {8}
  	};
	\end{tikzpicture}  
  }
    child {node [rectangle,draw] (a) {$2<x<4$}}
    child {node [rectangle,draw] (a) {$4<x<6$}}
    child {node [rectangle,draw] (a) {$6<x<8$}}
    child {node [rectangle,draw] (a) {$8<x<10$}}
  }  
  child {node [rectangle,draw] (c) {$10<x<12$}
  }
  child {node [rectangle,draw] (d) {$x>12$}
  };

\end{tikzpicture}

\end{center}
\end{tiny}

\end{frame}

\begin{frame} \frametitle{ApproxMWPaging Phase 3.}

\begin{tiny}
\begin{center}
\begin{tikzpicture}[
  scale = 0.6,
  level 1/.style = {sibling distance=10cm},
  level 2/.style = {sibling distance=6cm}, 
  level 3/.style = {sibling distance=6cm},
  level 4/.style = {sibling distance=3cm}
]
\node [arn_small] {10}
  child { node [arn_small] {2} 
    child {node [rectangle,draw] {$x<2$}    
    }
    child {node [arn_small] {6}
      child {node [arn_small] {4}
        child {node [rectangle,draw]{$2<x<4$}}
        child {node [rectangle,draw]{$4<x<6$}}    
      }
      child {node [arn_small]{8}
        child {node [rectangle,draw]{$6<x<8$}}
        child {node [rectangle,draw]{$8<x<10$}}    
      }
    }   
  }
  child {node [arn_small] {12}    
    child {node [rectangle,draw] {$10<x<12$}
    }
    child {node [rectangle,draw] {$x>12$}
    }  
  };
\end{tikzpicture}
\end{center}
\end{tiny}

\end{frame}

\begin{frame} \frametitle{ApproxMWPaging Phase 4.}

\begin{tiny}
\begin{center}
    \begin{tabular}{ | l | l | l | l |}
    \hline
    Memory Location & Node & Left Child Location & Right Child Location \\ \hline
    1  & 10  & 2    & 3    \\ \hline
    2  & 2  & 4    & 5    \\ \hline
    3  & 12 & 6    & 7    \\ \hline
    4  & $x < 2$  & ---   & ---    \\ \hline
    5  & 6  & 8   & 9   \\ \hline
    6  & $10 < x < 12$ & ---   & ---   \\ \hline
    7  & $x > 12$ & ---   & ---   \\ \hline
    8  & 4  & 10 & 11 \\ \hline
    9  & 8  & 12 & 13 \\ \hline
    10 & $2 < x < 4$  & --- & --- \\ \hline
    11 & $4 < x < 6$  & --- & --- \\ \hline
    12 & $6 < x < 8$  & --- & --- \\ \hline
    13 & $8 < x < 10$ & --- & 16   \\ \hline
    \end{tabular}
\end{center}

\end{tiny}
\end{frame}


\begin{frame} \frametitle{Expected Cost ApproxMWPaging} \label{45}
\begin{itemize}
\item[1.] We bind the depth of each key/gap based on its probability in the MWT 
\item[2.] We bind the depth of each key/gap based on its probability in BST
\item[3.] We bind the location in the memory of each key/gap (since we pack via BFS)
\item[4.] We can then explicitly bound the expected cost of search of our tree.
\end{itemize}
\end{frame}

\begin{frame}
BIND THE DEPTH IN THE MWT
\end{frame}

\begin{frame}
BIND THE DEPTH IN THE BST
\end{frame}

\begin{frame}
eXPLICIT UGLY COST
\end{frame}

\begin{frame}
nEAT TRICK, MUST BE LESS THAN $C_H*DEPTH$
\end{frame}

\begin{frame} \frametitle{Approximate Binary Search Trees of De Prisco and De Santis with Extensions by Bose and Dou\"{i}eb} \label{sec:deBST}

We provide another approach to building the approximately optimal BST under the HMM model. This approach uses the approximate BST solution (in the simple RAM model) of De Prisco and De Santos (modified by Bose and Dou\"{i}eb) \cite{de1993binary, bose2009efficient}. we explain the method here.

As in the classic Knuth problem, we are given a set of $n$ probabilities of searching for keys ($p_1, p_2, ..., p_n$), as well as $n+1$ probabilities of unsuccessful searches ($q_0, q_1, ..., q_n$). De Prisco and De Santos give an algorithm which constructs a binary search tree in $O(n)$ time with an expected cost of at most \cite{de1993binary}
\begin{align*}
H+1-q_0-q_n+q_{max}
\end{align*}
  where $q_{max}$ is the maximum probability of an unsuccessful search. This was later modified by Bose and Dou\"{i}eb (the same paper described in section~\ref{43}) to have an improved bound of \cite{bose2009efficient}
\begin{align*}
H + 1 - q_0 - q_n + q_{max} - \sum_{i=0}^{m'} pq_{\text{rank}[i]}.
\end{align*}
Here, $P$ is the number of increasing or decreasing sequences in a left-to-right read of the access probabilities of the leaves and $m'=\max({2n-3P,P})-1 \geq \frac{n}{2} - 1$.  Moreover, $pq_{\text{rank}[i]}$ is the $i^{th}$ smallest access probability among all keys and leaves except $q_0$ and $q_n$.

First, we explain the algorithm of De Prisco and De Santis and then explain the extensions of Bose and Dou\"{i}eb. De Prisco and De Santis' algorithm occurs in three phases.

\begin{itemize}
\item[\textbf{Phase 1}] An auxiliary probability distribution is created using $2n$ zero probabilities, along with the $2n+1$ successful and unsuccessful search probabilities. Yeung's linear time alphabetic search tree algorithm is used with the $2n+1$ successful and unsuccessful search probabilities used as leaves of the new tree created \cite{yeung1991alphabetic}. This is referred to as the \textit{starting tree}.

\item[\textbf{Phase 2}] What's known as the \textit{redundant tree} is created by moving $p_i$ keys up the \textit{starting tree} to the lowest common ancestor of keys $q_{i-1}$ and $q_i$. The keys which used to be called $p_i$ are relabelled to $old.p_i$.

\item[\textbf{Phase 3}] The \textit{derived tree} is constructed from the \textit{redundant tree} by removing redundant edges. Edges to and from nodes which represented zero probability keys are deleted. This \textit{derived tree} is a binary search tree with the expected search cost described.
\end{itemize}


In Bose and Dou\"{i}eb's work, they explain how they can substitute their algorithm for Yeung's linear time alphabetic search tree algorithm which results in a better bound (as described above). We use the updated version (by Bose and Dou\"{i}eb) of De Prisco and De Santis' algorithm as a subroutine in the sections to follow.
\end{frame}


\begin{frame} \frametitle{Algorithm ApproxBSTPaging}\label{Algorithm ApproxBSTPaging}

Our second solution to create an approximately optimal BST under the HMM model works as follows: \\

\begin{enumerate}
\item First, we create a BST $T$ using the algorithm of De Prisco and De Santis \cite{de1993binary} (as updated by Bose and Dou\"{i}eb \cite{bose2009efficient}). This takes $O(n)$ time. \\

\item In a similar fashion to step 4) of \textit{ApproxMWPaging}, we pack keys from $T$ into memory in a breadth-first search order starting from the root. This relatively simple traversal also takes $O(n)$ time.
\end{enumerate}

We are left with a binary search tree which is properly packed into memory in total time $O(n)$.
\end{frame}


\begin{frame} \frametitle{Expected Cost ApproxBSTPaging}\label{48}

As explained in the Bose and Dou\"{i}eb paper, the average path length search cost of the tree created by their algorithm is at most: \cite{bose2009efficient}
\begin{align*}
H + 1 - q_0 - q_n + q_{max} - \sum_{i=0}^{m'} pq_{\text{rank}[i]}
\end{align*}

We call this value $P_T$ (the average search cost of tree $T$). Similar to the proof in section~\ref{45}, if we can bound the cost of search for a given path length, then we can form a bound on the average cost of search in the HMM model. As before, we can describe the cost of searching for a key located at deepest node of tree $T$: $W$. Recall, $m'_j = \sum_{k \leq j} m_k$, $m'_0 = 0$ and let $h$ be the smallest $j$ such that $m'_j \geq n$.

\begin{lem} 
When using the ApproxBSTPaging, the cost of searching for a node deepest in the tree is at most:
\begin{align*}
W \leq \sum_{k=1}^{h-1} \left(\lfloor \lg(m'_i+1) \rfloor - \lfloor \lg(m'_{i-1}+1) \rfloor)\cdot c_i+ (D(T) - \lfloor \lg(m'_{h-1}+1) \rfloor \right)\cdot c_h
\end{align*}
\end{lem}

\begin{proof}
Since we are simply putting keys into memory in BFS order, and all we use is the height of the tree and the memory hierarchy, the proof is identical to that of Lemma~\ref{452}.
\end{proof}

Since we have the same result as Lemma~\ref{452}, this immediately implies Lemma~\ref{W<HT} is true as well. Assuming that $l \neq 1$, we have that $W<D(T)\cdot c_h$. As in the proof of the expected cost ApproxMWPaging, $\frac{W}{D(T)}$ represents the average cost per memory access when accessing the deepest (and most costly) element of our tree. Thus, $\frac{W}{D(T)}$ upper bounds the average cost per memory access when searching for any element of $T$.
\\

\begin{thm} \label{ApproxBSTThm}
\begin{align*}
C &\leq  (\frac{W}{D(T)}) \cdot  (H + 1 - q_0 - q_n + q_{max} - \sum_{i=0}^{m'} pq_{\text{rank}[i]}) \text{ and} \\
C &<  (H + 1 - q_0 - q_n + q_{max} - \sum_{i=0}^{m'} pq_{\text{rank}[i]})\cdot c_h
\end{align*}
\end{thm}

\begin{proof}

Bose and Dou\"{i}eb show that after using their algorithm for Phase 1 of De Prisco and De Santis algorithm algorithm, every leaf of the \textit{starting tree} (all keys representing successful searches and gaps representing unsuccessful searhces) are at depth at most $\lfloor \lg(\frac{1}{p}) \rfloor + 1$  for at least $\max(2n-3P,P)-1$ of $p \in \left( \{p_1, p_2, ..., p_n \} \cup \{ q_0, q_1, ..., q_n \} \right)$ and $\lfloor \lg(\frac{1}{p}) \rfloor + 2$ for all others. Recall that $P$ is the number of peaks in the probability distribution $q_0, p_1, q_1, ..., p_n, q_n$. After phases 2 and 3 of the algorithm, each key has its depth decrease by 2, and all leaves (except one) move up the tree by one.
\begin{align*}
C &\leq \sum_{i=1}^{n} \left(p_i\cdot \frac{depth(p_i)+1}{D(T)}\cdot  W \right)+ \sum_{i=0}^{n} \left(q_i\cdot \frac{(depth(q_i))}{D(T)}\cdot  W \right) \\
\implies C &\leq \frac{W}{D(T)} \left(\sum_{i=1}^{n}(p_i\cdot (depth(p_i)+1))+ \sum_{i=0}^{n}(q_i\cdot (depth(q_i))) \right) \\
\implies C &\leq \frac{W}{D(T)} \left(\text{WeightedAveragePathLength}(T) \right) \\
\implies C &\leq  \left( \frac{W}{D(T)} \right) \cdot  \left(H + 1 - q_0 - q_n + q_{max} - \sum_{i=0}^{m'} pq_{\text{rank}[i]} \right) \text{ and by Lemma~\ref{W<HT}} \\
\implies C &<  \left(H + 1 - q_0 - q_n + q_{max} - \sum_{i=0}^{m'} pq_{\text{rank}[i]} \right)\cdot c_h
\end{align*}

\end{proof}
\end{frame}


\begin{frame} \frametitle{Improvements over Thite in the HMM$_2$ Model}

The HMM$_2$ model is the same as the general HMM model with the added constraint that there are only two types of memory (slow and fast). In Thite's thesis, he proposed both an optimal solution to the problem, as well as an approximate solution (\textit{Algorithm Approx-BST}) that runs in time $O(n \lg(n))$ \cite{thite2008optimum}. we first show that:
\begin{lem}
The proof of quality of approximation of Thite's approximate algorithm has a small mistake which raises its cost from at most: 
\begin{align*}
&c_2(H+1) \text{ to} \\
&c_2(H+1+\sum_{i=0}^{n}q_i)
\end{align*}
\end{lem}

\begin{proof}
Specifically, in Lemma 14 in 3.4.2.3 Quality of approximation in Thite's thesis, he proves that $\delta(z_k) = l+2$. Here, $\delta(z_k)$ represents the depth of a leaf node $z_k$. Note that Thite considers the depth of the root to be $1$ instead of $0$ which updates how the cost of search is calculated accordingly. $l$ represents the depth of recursion of the \textit{Approx-BST} algorithm. In Lemma 15, Thite goes on to prove that $q_k \leq 2^{-\delta(z_k)+2}$. In this proof, Thite shows that $q_k \leq 2^{-l+1}$, but makes a mistake when substituting in $l=\delta(z_k)-2$ and gets $q_k \leq 2^{-\delta(z_k)+2}$ while the correct bound is $q_k \leq 2^{-\delta(z_k)+3}$. This updated bound would change his depth bound in Lemma 16 from $\delta(z_k) \leq \lfloor \lg(\frac{1}{q_k}) \rfloor + 2$ to $\delta(z_k) \leq \lfloor \lg(\frac{1}{q_k}) \rfloor + 3$. Finally, substituting into his final equation for the upper bound on the expected cost of search for the tree would give:

\begin{align*}
&\sum_{i=1}^{n} \left(c_2 p_i \delta(B_i )+ \sum_{i=0}^{n} c_2 q_i (\delta(q_i)-1 \right)\\
\leq &\sum_{i=1}^{n} \left(c_2 p_i (\lg(\frac{1}{p_i})+1)+ \sum_{i=0}^{n} c_2 q_i (\frac{1}{q_i}-1+3 \right) \\
\leq &c_2 \cdot \left(H+1+\sum_{i=0}^{n}q_i \right)
\end{align*}
\end{proof}

This is of particular interest because if Thite's bound on \textit{Algorithm Approx-BST} had been correct, then in the case where $c_2=c_1$ (typical RAM model), Thite's method would have provided a strict improvement over the work of Bose and Dou\"{i}eb \cite{bose2009efficient} which seems unlikely since Thite used the BST approximation algorithm of Mehlhorn from 1984 \cite{mehlhorn1984sorting} (much before the work of Bose and Dou\"{i}eb).

By simply substituting in for $l=2$ we immediately get that, under this HMM$_2$ model, both ApproxMWPaging and ApproxBSTPaging provide strict improvements over Thite's \textit{Algorithm Approx-BST}. 

\begin{thm}
In the HMM$_2$ model, ApproxMWPaging has an expected cost of at most \\
\begin{align*}
C_{ApproxMWPaging} < (H + 1 + \sum_{i=0}^n q_i - q_0 - q_n - \sum_{i=0}^m q_{\text{rank}[i]}) \cdot  c_2
\end{align*}
and ApproxBSTPaging has an expected cost of at most
\begin{align*}
C_{ApproxBSTPaging} <  (H + 1 - q_0 - q_n + q_{max} - \sum_{i=0}^{m'} pq_{\text{rank}[i]})\cdot c_2.
\end{align*}
ApproxMWPaging does so in $O(n\cdot \lg(m_1))$ while ApproxBSTPaging runs in $O(n)$.
\end{thm}

\begin{proof}
We can directly sub $l=2$ into Theorems~\ref{ApproxMWPagingThm} and~\ref{ApproxBSTThm} to get the desired result (the running times are as explained in sections~\ref{Algorithm ApproxMWPaging} and~\ref{Algorithm ApproxBSTPaging}).
\end{proof}

Since both ApproxMWPaging and ApproxBSTPaging run in time $o(n\lg(n))$ (the time of Thite's \textit{Approx-BST} algorithm) and we can see that:\\
\begin{align*}
C_{ApproxMWPaging} &< (H + 1 + \sum_{i=0}^n q_i - q_0 - q_n - \sum_{i=0}^m q_{\text{rank}[i]}) \cdot  c_2 \\
&< c_2(H+1+\sum_{i=0}^{n}q_i) \\
 C_{ApproxBSTPaging} &< (H + 1 - q_0 - q_n + q_{max} - \sum_{i=0}^{m'} pq_{\text{rank}[i]})\cdot c_2 \\
 &< c_2(H+1+\sum_{i=0}^{n}q_i)
 \end{align*}

both methods provide a strictly better approximation and run faster than the \textit{Algorithm Approx-BST} of Thite.
\end{frame}


%=========================================
\section{Optimal BSTs On Unordered Multisets}\label{BST over Multisets}

\begin{frame}
In this chapter we examine a problem related to the initial optimal binary search tree problem of Knuth \cite{knuth1971optimum}. Our keys will no longer have a strict ordering and can be rearranged as we please before constructing our binary search tree. This problem is very similar to constructing optimal prefix-free binary codes which is solved using a Huffman coding. This problem differs because, (as in previous chapters) when searhing, we charge 1 for each internal node we examine but do not charge for leaf nodes (since we do not need to "examine" them).
\end{frame}


\begin{frame} \frametitle{The Multiset Binary Search Tree Problem}\label{The Multiset Binary Search Tree Problem}

 Consider a multiset (a set with possible duplicate values) of $n$ probabilities: $M = \lbag p_1, p_2, ..., p_n \rbag$ such that $\sum\limits_{i=1}^n p_i = 1$. Our goal is to create a tree $T$ which minimizes the expected path length, $P_T$, of nodes (the expected cost of our search in comparisons):
\begin{equation}
P_T = \sum_{i=1}^{n} p_i(b_i+1) - \sum_{i \in L}p_i
\end{equation}
Here, $b_i$ is the \emph{depth} of the $i$'th key of $M$ and $L$ is the set of leaves of the tree. We subtract the weight of the leaves of the tree since we need one less comparison to return a pointer a leaf node (as in the original optimal BST problem). In order to simplify our proof later we define $f_T$ (the working \emph{depth} of a node in tree $T$) as follows:
\begin{center}
\[
    f_T(p_i)= 
\begin{cases}
    b_i+1,& \text{if the } i \text{'th key is an internal node}\\
    b_i,              & \text{otherwise}.
\end{cases}
\]
\end{center}
Our expected path length can then be re-written as:
\begin{align*}
P_T = \sum_{i=1}^{n} p_i\cdot f_T(p_i).
\end{align*}
\end{frame}


\begin{frame} \frametitle{The OPT-MSBST Algorithm}\label{OPT-MSBST}

We propose the following simple algorithm entitled \textit{OPT-MSBST} for solving this multiset binary search tree problem and subsequently show that it is optimal.

\begin{enumerate}
\item First, we create a vector $R$ which is equal to the sorted (from largest to smallest) multiset $M$.

\item We create a BST $T$ as follows. The root of our tree will be $R[0]$, its two children will be $R[1]$ and $R[2]$, and so on. Formally, $R[i]$ will be placed at location $i+1$ in the BFS order of $T$.
\end{enumerate}

The complete proof of the optimality of this algorithm follows a similar form to the proof that Huffman codes are optimal \cite{huffman1952method}. First, we introduce a useful lemma.

\begin{lem}\label{MS Swap Lemma}
Let $T$ be a BST created for a multiset of probabilities as described in the problem definition in~\ref{The Multiset Binary Search Tree Problem}. Let $T'$ be the tree created by swapping the node locations of $p_j$ and $p_k$ in $T$. Then,
\begin{align*}
P_{T'}-P_T = (p_j - p_k)\cdot (f_T(p_k)-f_T(p_j)).
\end{align*}
\end{lem}

\begin{proof}
We let $f_{T'}$ represent the working height of a node in $T'$.
\begin{align*}
P_{T'}-P_T &= \sum_{i=1}^{n} p_i\cdot f_{T'}(p_i) - \sum_{i=1}^{n} p_i\cdot f_T(p_i) \\
&= p_j\cdot f_{T'}(p_j)+p_k\cdot f_{T'}(p_k) - p_j\cdot f_T(p_j) - p_k\cdot f_T(p_k)\\
&= p_j\cdot f_T(p_k)+p_k\cdot f_T(p_j) - p_j\cdot f_T(p_j) - p_k\cdot f_T(p_k)\\
&= p_j\cdot (f_T(p_k)-f_T(p_j)) + p_k \cdot (f_T(p_j) - f_T(p_k))\\
&= p_j\cdot (f_T(p_k)-f_T(p_j)) - p_k \cdot (f_T(p_k)-f_T(p_j))\\
&= (p_j - p_k)\cdot (f_T(p_k)-f_T(p_j))
\end{align*}

\end{proof}

Next, we prove the optimality of this algorithm. 

\begin{lem}\label{MSSolvesOpt}
The tree $T$ created by \textit{OPT-MSBST} for the multiset of probabilities $p$ solves the multiset binary search tree problem optimally.
\end{lem}

\begin{proof}
We prove this by inducting on $n = |M|$, the size of our multiset of probabilities. Consider the case where $n = 1$. In this case, we create the only tree possible which is obviously optimal. Suppose all trees created by \textit{OPT-MSBST} with $k-1$ nodes are optimal. Consider the case where $n=k$. Let $p_{min} \in M$ be the probability of the node placed in the final position in BFS order in $T$. By the definition of \textit{OPT-MSBST}, $p_{min}=\min_{p_i \in M}$.

\noindent We define $M'$, a multiset of probabilities of size $k-1$ with probabilities $\lbag p'_1, p'_2, ..., p'_{k-1} \rbag$ such that
\begin{align*}
p'_i=\frac{p_i}{1-p_{min}} \implies p_i = p'_i\cdot (1-p_{min}).
\end{align*}

\noindent Note that
\begin{align*}
\sum\limits_{p'_i \in M'} &= \sum_{i : p_i \in (M - \{p_{min}\})}p'_i\\
 &= \sum_{i : p_i \in (M - \{p_{min}\})}\frac{p_i}{1-p_{min}}\\
 &= \frac{1}{1-p_{min}}\cdot \sum_{i : p_i \in (M - \{p_{min}\})}p_i\\
 &= \frac{1}{1-p_{min}}\cdot (1-p_{min})\\
&= 1.
\end{align*}

Thus, let $T'$ be the tree created for $M'$ using \textit{OPT-MSBST}. By our induction hypothesis, it is optimal. Consider using \textit{OPT-MSBST} to create $T$ for $M$. During the running of the algorithm, we assume  we use an arbitrary but fixed tie-breaking rule during sorting. Because of this rule, and by the definition of \textit{OPT-MSBST}, for all $p'_i \in M'$, $f_{T'}(p'_i)=f_T(p_i)$. Now, we consider the cost of our tree $T$:
\begin{align*}
P_T &= \sum_{i:p_i \in (M - \{p_{min}\})} p_i\cdot f_T(p_i) + p_{min}\cdot f_T(p_{min}) \\
&= \sum_{i:p_i \in (M - \{p_{min}\})} p'_i\cdot (1-p_{min})\cdot f_{T'}(p_i) + p_{min}\cdot f_T(p_{min}) \\
&= (1-p_{min})\cdot \sum_{p'_i \in M'} p'_i\cdot f_{T'}(p_i) + p_{min}\cdot f_T(p_{min}) \\
&= (1-p_{min})\cdot P_{T'} + p_{min}\cdot f_T(p_{min})
\end{align*}



Suppose for contradiction that $T$ is not optimal, and there exists a better tree $Z$ (which is optimal). We let $f_Z$ represent the working depth of a node in $Z$. Note that there must exist an optimal tree $Z$ with a $p_i \in M$ such that $f_Z(p_i)$ is maximized over all $p_i \in M$, and $p_i = min_{p_j \in M}$. Otherwise, by Lemma~\ref{MS Swap Lemma}, we can swap the locations of the nodes $p_i$ (the smallest probability) and $p_j$ (the deepest node in the tree) and get a tree at least as good (if not better). We consider such a tree $Z$ and let $p_{min}$ be its minimum probability (which has maximum $f_Z$ value).

We let $Z'$ be tree made from $Z$ by removing $p_{min}$ from the tree over the probability distribution $M'$ as in the proof of Lemma~\ref{MSSolvesOpt}. As before,
\begin{align*}
p'_i=\frac{p_i}{1-p_{min}} \implies p_i = p'_i\cdot (1-p_{min}).
\end{align*}
We now describe how we remove $p_{min}$ to make $Z'$. If $p_{min}$ is a leaf, we simply take out the leaf. If it is an internal node (it must be the parent of a leaf since it has maximum $f$ value) we simply move one of its children (call it $l$), put it in $p_{min}$'s place, and make its possible other child (call it $r$) the child of $l$. Consider the cost of $Z$:
\begin{align*}
P_Z &= \sum_{p_i \in M} p_i\cdot f_Z(p_i) \\
&= \sum_{i:p_i \in (M - \{p_{min}\})} p_i\cdot f_Z(p_i) + p_{min}\cdot f_z(p_{min}) \text{ as in~\ref{MSSolvesOpt}, we have that:} \\
&= \sum_{p_i \in M'} (1-p_{min})\cdot p'_i\cdot f_Z(p_i) + p_{min}\cdot f_Z(p_{min}) \\
&= (1-p_{min})\cdot P_{Z'} + p_{min}\cdot f_Z(p_{min}) \\
\end{align*}

Note that our algorithm creates a balanced binary search tree, so the maximum value of $f_T$ over $M$ for $T$ cannot be greater than the maximum value of $f_Z$ for $Z$. Since we know that $p_{min}$ has has the maximum value for $f_T$ and for $f_Z$,  we have that $f_T(p_{min}) \leq f_Z(p_{min})$. Thus, we have
\begin{align*}
P_Z &< P_T \\
\implies (1-p_{min})\cdot P_{Z'} + p_{min}\cdot f_Z(p_{min}) &< (1-p_{min})\cdot P_{T'} + p_{min}\cdot f_T(p_{min}) \\
\implies P_{Z'} &< P_{T'} \\
\end{align*}
Which is a contradiction to the optimality of $T'$ over $k+1$ nodes. Thus, $T$ must be optimal as desired.


\end{proof}

We are now ready to prove our main theorem.

\begin{thm}
The tree $T$ created by \textit{OPT-MSBST} for the multiset of probabilities $p$ solves the multiset binary search tree problem optimally and is unique up to permutation of the assignment of nodes which have the same $f_T$ value and permutation of the assignment of nodes which correspond to the same probability.
\end{thm}

\begin{proof}
Since we already know that \textit{OPT-MSBST} provides an optimal solution, all that remains is to show that the tree $T$ (created for probability multiset $M$ with working node level function $f_T$) is unique up to the permutations described. Consider any optimal tree $Z$ for probability multiset $M$. Suppose their exists $p_i \in M$ and $p_j \in M$ such that $p_i < p_j$ and $f_Z(p_i) < f_Z(p_j)$. By Lemma~\ref{MS Swap Lemma} swapping $p_i$ and $p_j$ gives a strictly better tree, a contradiction. Thus, no such $p_i$ and $p_j$ must exist. This exactly means that $Z$ is identical to $T$ up to permutations between identical probabilities and within the same values of $f_T$ as required.
\end{proof}
\end{frame}


%=========================================



\section{Conclusion and Open Problems} \label{Conclusion and Open Problems}

\begin{frame} \frametitle{Conclusion}

In this work, we examined several problems related to the optimum BST problem originally proposed (and solved) by Knuth in 1971 \cite{knuth1971optimum}. In Chapter~\ref{An Improved Bound for the Modified Minimum Entropy Heuristic} we showed that the Modified Entropy Rule first proposed by  G{\"u}ttler, Mehlhorn and Schneider in 1980 had a worst case expected cost of $H+4$. This improved upon the previous best bound of $c\cdot H+2$ where $c \approx 1.08$ \cite{guttler1980binary}.

 In the next two chapters, we examined the problem under different models for external memory. In Chapter~\ref{Approximate Binary Search in the Hierarchical Memory Model} we showed that under the Hierarchical Memory Model (HMM) by Aggarwal et al. Our two algorithms ApproxMWPaging and ApproxBSTPaging solved the problem in time $O(n\cdot \lg(m_1))$ and $O(n)$ respectively \cite{aggarwal1987model}. Moreover in sections~\ref{45} and~\ref{48}, we showed the two solutions had worst case expected costs strictly less than
\begin{align*}
&(H + 1 + \sum_{i=0}^n q_i - q_0 - q_n - \sum_{i=0}^m q_{\text{rank}[i]}) \cdot  c_h \text{ and} \\
&(H + 1 - q_0 - q_n + q_{max} - \sum_{i=0}^{m'} pq_{\text{rank}[i]})\cdot c_h \text{ respectively  (theorems~\ref{ApproxMWPagingThm} and~\ref{ApproxBSTThm})}.
\end{align*}
   We concluded by showing a mistake in the Master's thesis of Thite, and subsequently proving our solutions provided an improvement over Thite's in the related HMM$_2$ model.

In Chapter~\ref{BST over Multisets}, we considered the optimum BST problem without explicit ordering on the keys. This essentially left us with a multiset of probabilities over which we attempted to build an optimum BST. In section~\ref{OPT-MSBST}, we described an algorithm, OPT-MSBST, and proved that it solved the problem optimally. We also showed the solution was unique up to certain permutations.

We have tabulated these results with novel contributions in bold.

\begin{table}[!hb]


\begin{center}
    \begin{tabular}{ | l | l | l | p{7.1cm} |}
    \hline
    Algorithm & Model & Running Time & Worst case expected cost \\ \hline
    \scriptsize Modified Minimum Entropy & \scriptsize RAM  & \scriptsize $O(n^2)$    & \scriptsize $\mathbf{C \leq H+4}$    \\ \hline
   \scriptsize \textbf{ApproxMWPaging}  & \scriptsize HMM  & \scriptsize $\mathbf{O(n\cdot lg(m_1))}$   & \scriptsize $\mathbf{C < (H + 1 + \sum_{i=0}^n q_i - q_0 - q_n - \sum_{i=0}^m q_{\text{rank}[i]}) \cdot  c_h}$    \\ \hline
    \scriptsize \textbf{ApproxBSTPaging}  & \scriptsize HMM & \scriptsize $\mathbf{O(n)}$    &  \scriptsize $\mathbf{C < (H + 1 - q_0 - q_n + q_{max} - \sum_{i=0}^{m'} pq_{\text{rank}[i]})\cdot c_h}$    \\ \hline
    \scriptsize \textbf{OPT-MSBST}  & \scriptsize RAM  & \scriptsize $\mathbf{O(n\cdot lg(n))}$    & \scriptsize ---    \\ \hline
    \end{tabular}
\end{center}

\caption{Models, running rimes, and worst case expected costs for algorithms discussed in this thesis.}
\end{table}
 
Several interested and related problems still remain open. Firstly, is $H+4$ a tight bound for the Modified Entropy Rule? We conjecture that this is not the case, and we believe the bound could be lowered to $H+2$. Moreover, while the metric used to search for the root in this heuristic is good, it is definitely not perfect. The root chosen (up to the modifications of the rule) has the maximum 3-way entropy split. However, this is not necessarily the best root, as selecting larger probability keys as the root can decrease the cost of the tree. It would be interesting to consider what the correct metric would be for correctly selecting the root, possibly in a greedy manner. Chapter~\ref{BST over Multisets} is ultimately an introduction into considerations for this problem. 
Finally, the work in Chapter~\ref{Approximate Binary Search in the Hierarchical Memory Model} can likely be extended to more recent models for external memory. 

\end{frame}

\bibliographystyle{abbrv}
\bibliography{ProjectPresentation}


\end{document}
